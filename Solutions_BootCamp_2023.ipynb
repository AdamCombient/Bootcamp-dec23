{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkZyvXSmxl3S"
   },
   "source": [
    "# Generative AI - SEBx & Combient Hackathon December 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sF2Up6-RmhA"
   },
   "source": [
    "This notebook provides some examples and exercises for dealing with Large Language Models via an API interface. \n",
    "\n",
    "<br />\n",
    "\n",
    "In Hands-On 1 We provide examples of how to interact with the API and how to start exploring concepts related to Prempt Engineering, the art of communicating with Large Language Models.\n",
    "\n",
    "In Hands-On 2 we cover some basic aspects of vector embeddings; such as how to download models from the Hugging Face space and how to embed text into semantic vector embeddings. We also show how you can use the vector store library such as FAISS to directly embed and store embedded vectors via an in-memory index for semantic similarity search. In addition we demonstrate how a fine-tuning procedure of embedding models can be set up to directly tune the model for specific data and purposes.\n",
    "\n",
    "In Hands-On 3 we set up a simple RAG system for asking queries of PDF's. This can become exponentially difficult depending on the knowledge base and what you require as a user. Here we restrict to simple examples and a somewhat limited scope of application.\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "***\n",
    "\n",
    "**Note:**\n",
    "This notebook contains code written in [Python](https://www.python.org/), which is a commonly used programmming language. No advanced coding expertise is required for completing the exercises and clear instructions are provided for when and where some modifications are required from the user. The notebook itself is a [Jupyter notebook](https://jupyter.org/), which provides a graphical interface for writing and executing Python code as well as displaying the output. The cell blocks in this notebook are either text formatted via the [Markdown langugage](https://en.wikipedia.org/wiki/Markdown) or executable code snippets written in Python.\n",
    "\n",
    "The box below provides instructions for executing the code within these cell blocks and the result of executing code will be displayed beneath the relevant code cell block.\n",
    "\n",
    "\n",
    "In order to run this notebook properly you will need\n",
    "\n",
    "* **Gmail account** - Follow the provided instructions to download the notebook to your GDrive, so that you can edit and save it freely.\n",
    "* **Google GDrive access** - enable the app `Google Colaboratory`. The process is highlighted in the accompanying reading material, but you can also ask ChatGPT a question like this by posing the following query: <span style=\"color:blue;\">How do I enable google colab for the first time?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FflnrWlYmoZ"
   },
   "source": [
    "> **`Run and execute each code cell block in the notebook in a consecutive manner. This is important since some code cell blocks relies on having properly executed some previous code cell block.`**\n",
    ">\n",
    ">\n",
    "> Notebook code blocks can be executed via either:\n",
    "> * **shift + enter**: executes current code block and moves to the cell below\n",
    "> * **control + enter**: executes current code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uczbePAtomja"
   },
   "source": [
    "## Environment setup\n",
    "\n",
    "Here we set up the environment and make sure we can access data via Google Drive. Run the below code blocks to install necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVAlM-i8pnK7"
   },
   "source": [
    "Run the code below to clone the GenAI_BootCamp2023 directory which contains files which we will be using during this course. The code clones a directory called GenAI_BootCamp2023 from our Github repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a83ptAEpnK7"
   },
   "source": [
    "> **`During execution of this cell block you will be prompted to provide your GDrive access to download the course content. Use your Google account credentials to allow this action.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are not using google colab, change `use_drive` to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_drive = True\n",
    "\n",
    "if use_drive:\n",
    "  import os\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  # This moves into the drive where the course content folder will be downloaded\n",
    "  %cd /content/drive/MyDrive\n",
    "\n",
    "  repo_path = 'bootcamp_dec7'\n",
    "  repo_url = 'https://github.com/banque-nouveau/bootcamp_dec7.git'\n",
    "\n",
    "  # Check if the repo directory exists\n",
    "  if os.path.isdir(repo_path):\n",
    "    # If it exists, then pull any changes from the remote repository\n",
    "    %cd {repo_path}\n",
    "    !git pull\n",
    "    %cd ..\n",
    "  else:\n",
    "    # If the directory does not exist, clone the repository\n",
    "    !git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rz1vY1ZpnK8"
   },
   "source": [
    "You can now view the files by clicking on the directory folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkgdiTzvpnK9"
   },
   "source": [
    "## Packages & Imports\n",
    "\n",
    "Installing and importing necessary packages/modules. Note that these are only installed in a virtual environment when using the Colab Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAsxW33ZpnK9"
   },
   "source": [
    "Run the below code block to install some of the Python libraries which are required for running the Notebook.\n",
    "\n",
    "Note that this may take up to ~15 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mHgu0eCpnK-",
    "outputId": "4354cbaf-17fb-4436-9bd7-f136b6b8fc13"
   },
   "outputs": [],
   "source": [
    "!pip install -q cohere \\\n",
    "    -q tiktoken \\\n",
    "    -q langchain \\\n",
    "    -q sentence_transformers \\\n",
    "    -q openai \\\n",
    "    -q faiss-cpu \\\n",
    "    -q colorama \\\n",
    "    -q pypdf \\\n",
    "    -q PyMuPDF \\\n",
    "    -q requests \\\n",
    "    -q beautifulsoup4 \\\n",
    "    -q umap-learn \\\n",
    "    -q mycolorpy \\\n",
    "    -q pandas \\\n",
    "    -q plotly \\\n",
    "    -q seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erLd7QWNpnK-"
   },
   "source": [
    "Run the below code block to import the necessary library modules used in the notebook.\n",
    "\n",
    "Note that this may take up to ~20 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plhTcriIpnK-"
   },
   "outputs": [],
   "source": [
    "# Some system and base modules\n",
    "import os\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "from typing import List, Optional, Type\n",
    "import getpass\n",
    "\n",
    "# NLP modules\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, JSONLoader\n",
    "import fitz\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Other modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "\n",
    "# modules for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from mycolorpy import colorlist as mcp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2ZpOqHNEbdp"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(16,10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRf63DDVq_a6",
    "outputId": "b7134982-148b-4a37-96be-6105179d0df0"
   },
   "outputs": [],
   "source": [
    "print(f\"OpenAI version: {openai.__version__}\")\n",
    "print(f\"Langchain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLtHvH5ZpnK-"
   },
   "source": [
    "## Setting the access key for OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtTEx0QSpnK-"
   },
   "source": [
    "> **NB: Don't share the OpenAI access key in public spaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHjg3TzspnK_"
   },
   "source": [
    "> **The OpenAI API key can be set manually in the notebook by running the code cell block below.**\n",
    ">\n",
    "> **`A query box will appear the first time you run the below code cell block. Paste the OpenAI API key which you have been provided into the query box and press Enter/Return (access key is on the form sk-...)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5fmIpfSpnK_",
    "outputId": "d942846d-97a2-43d1-8300-c3c302e542e7"
   },
   "outputs": [],
   "source": [
    "# Here we can set the OpenAI API access key manually in case it fails to load from the environment.\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  api_key = getpass.getpass(\"Enter OpenAI API Key here\")\n",
    "  os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "else:\n",
    "  print(f\"OPENAI_API_KEY fetched from environment!\")\n",
    "\n",
    "\n",
    "# sk-..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQJWnMDwtU5k"
   },
   "outputs": [],
   "source": [
    "# You can optionally manually insert the OpenAI API key below between the quotation marks.\n",
    "# Then uncomment the following two lines by removing the preceeding # and run the cell\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "#print(f\"The Open AI access key is given by: \\n\\n {os.environ['OPENAI_API_KEY']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d48MRBCTuDQI"
   },
   "outputs": [],
   "source": [
    "if use_drive:\n",
    "  # The following helps to format print output to match the size of the broser window\n",
    "  from IPython.display import HTML, display\n",
    "\n",
    "  def set_css():\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "      pre {\n",
    "          white-space: pre-wrap;\n",
    "      }\n",
    "    </style>\n",
    "    '''))\n",
    "  get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q97YM4YIncHT"
   },
   "source": [
    "# Hands-On 1: Interfacing with OpenAI API, Prompt Engineering & Internet access for Extended LLM Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm3qcwQ77OK6"
   },
   "source": [
    "This section provides a hands-on introduction to API interactions with the OpenAI GPT models and Prompt Engineering for end users with a basic understanding of the Python programming language. No advanced coding or technical background knowledge of generative AI or LLM is required for completing the exercises in the notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<b>Example themes for exercises </b>:\n",
    "\n",
    "* We will see how to interact with the OpenAI models via API. This includes simple one-turn interactions and concatenating conversations for follow up queries.\n",
    "\n",
    "* Basic Prompt Engineering techniques which introduces the lkey concepts for enabling productive interactions with LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOccDkGWpnK_"
   },
   "source": [
    "## Interacting with OpenAI models via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPiY-aU8pnK_"
   },
   "source": [
    "**Why should you use API?**\n",
    "\n",
    "Using the OpenAI API (Application Program Interface) to access LLM models gives us several advantages over the browser version, especially when we want to build custom made applications. One of the big perks of using the API is automation. Python, as a common programming langauge, is great at taking care of repeating or tricky tasks, which is helpful when we have a lot of such tasks to handle.\n",
    "\n",
    "With the help of python we can also get the OpenAI models to work well with other tools and software we have. Plus, it lets us adjust things to work the way we want, making our interaction with the OpenAI models more tailored.\n",
    "\n",
    "Additionally, using the API helps us keep track of any changes made along the way, which is great for collaborative team work. This also ensures that every time we run a task, it gives us consistent results, making it easier to check if something goes wrong and fix it. This reliability helps make our solutions stronger and more dependable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zRlPPEcpnK_"
   },
   "source": [
    "We will be making use of a wraper from the `langchain` code library to call on the OpenAI models. This provides a simple and relatively user-friendly way to interact with the model, making it a great choice for beginners or for those looking to get things done quickly. This method provides us access to a `temperature` parameter, which determines the randomness or stochastic nature of the output the model will give us. A temperature of 1 makes the model more random, while a temperature close to 0 makes the model more deterministic with the same output for repeated queries. Empirical studies have shown that most people prefer the output to be somewhat creative in nature with a temperature value of around 0.7, which is the default value set for the OpenAI models. You can try changing this value and experience how the output changes for the same query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HURGvwIhpnLA"
   },
   "source": [
    "In the code snippet below we call on the API to create a chat_model instance. Model is specified via the `model` parameter on the second row. A complete list of available models to choose from can be found [here](https://platform.openai.com/docs/models/overview). We will start by using one of the GPT-3.5 turbo models.\n",
    "\n",
    "* **gpt-3.5-turbo** uses the latest available model version of GPT-3.5. Allows for up to **16,385 tokens** as input and output\n",
    "    * **gpt-3.5-turbo-0613** is a legacy version of GPT-3.5, currently the same as above. Will be replaced Dec 11 2023 and be depricated on June 13 2024. Has a **4,096 token** limit\n",
    "    * **gpt-3.5-turbo-1106** is the latest version of GPT-3.5, with enhanced capabilities and a **16,385 token** limit. Will become default model after Dec 11 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_3Q2_WxFpnLA",
    "outputId": "5cce203b-f3fb-42ec-9ed0-46aeac1e7983"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FJq0chtpnLA"
   },
   "source": [
    "> **NB 1: The GPT-3.5 model usually only takes seconds to respond to a query. If it takes more than half a minute, stop the execution and rerun the cell block.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GxqGDgcpnLA"
   },
   "source": [
    "> **NB 2: This is one option for implementing the LLM call, which is nice due to its simple and intuitive format. At the bottom of this section we explore using the OpenAI API directly as an alternative option, which provides even greater control of the input/output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hi6JKTTpnLA"
   },
   "source": [
    "The `chat_model` instance accepts a list of `messages` as input. We specify messages according to a schema used by various functions in the `langchain` library. This will facilitate some of the more advanced functionalities we will make use of later. Messages in a chat conversation are generally provided as a `list` (inside square brackets) of conversation turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "o3rZ9H_IpnLA",
    "outputId": "663a7c81-3719-41bf-9baa-ffc670a963f9"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    # Overall system behaviour of the chat bot\n",
    "    SystemMessage,\n",
    "    # User message, query to chat bot\n",
    "    HumanMessage,\n",
    "    # The response from the chat bot, for conversational turns\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "\n",
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "# The user prompt is what would be written in the chat interface, your query\n",
    "user_prompt = \"What do you know about OpenAI's mission statement?\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "sshnxEgVpnLA",
    "outputId": "09a19421-9f2f-4839-b6be-785ea0774e50"
   },
   "outputs": [],
   "source": [
    "# We can check what the messages looks like by printing it out\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q-A2UYtpnLA"
   },
   "source": [
    "As we see, the `messages` variable is a list containing our `SystemMessage` and `HumanMessage`.\n",
    "\n",
    "\n",
    "Once we have a list of messages in the above format it is easy to call the OpenAI model and get a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrWnfH56pnLA"
   },
   "source": [
    "> **`Calling on the OpenAI models relies on connecting to the OpenAI server. This can occasionally be slow `**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "dNqqUUcKpnLA",
    "outputId": "3e657732-d481-41f1-a5e7-6cc82182d840"
   },
   "outputs": [],
   "source": [
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HobuvoQdpnLB"
   },
   "source": [
    "The full response is actually formatted as a `AIMessage`, as we can see by printing out the full response without using the print function. Observe that if we do not use the formatting of the `print` function you will see linebreak characters such as `\\n` appearing in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "saxZt4eMpnLB",
    "outputId": "7adb7634-7bb0-4ed9-a617-04e7a6d4cd65"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcSWf3BIpnLG"
   },
   "source": [
    "### Giving ChatGPT a short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C095gg8lpnLG"
   },
   "source": [
    "We can incorporate the `AIMessage` response in a new series of messages and ask a follow up question if you wish. This provides the illusion of the chat model having a memory and being able to continue a conversation for a few turns, as you have seen while working in the browser environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "khsPYJQNpnLG",
    "outputId": "c1e503f1-ff2e-4c62-8c11-37283196b3e7"
   },
   "outputs": [],
   "source": [
    "# Let's ask a follow-up question which refers to the previous conversation without explicitly mentioning e.g., Patricia Industries\n",
    "follow_up_question = \"Can you say something more about the mission statement?\"\n",
    "\n",
    "# We can now add the response we got from the first query together with our follow-up question\n",
    "# Note that if we did not include the first message, the reference to that would be missing in this follow-up\n",
    "# In python, you can add lists together to form a new list which makes it easy to add new conversation turns into the messages variable\n",
    "# Not that we add both the chat bot response and our follow up question\n",
    "messages = messages + [response, HumanMessage(content=follow_up_question)]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "DMjPY9-8pnLG",
    "outputId": "e0969f50-17c5-4c20-8fa9-09edbf8b656f"
   },
   "outputs": [],
   "source": [
    "# Here we collect the ouput from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF__1A8TpnLG"
   },
   "source": [
    "By continuing in this fashion we can record a conversation history which we send to the chat model as long as the total text content does not exceed the models content limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G-1ULuhpnLG"
   },
   "source": [
    "### OPTIONAL: Using the OpenAI API directly instead of the `langchain` wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQupUM2EpnLG"
   },
   "source": [
    "The below code snippet calls the OpenAI API directly, which allows us to access **all** of the available input and output options. While this approach offers a higher degree of control, it may not be as easy to use or straightforward to interpret as the functions from langchain. If you are interested you can try it out on your own and test various parameters. For example, the `top_p` parameter is another way to control the randomness of the model output. The recomendation from OpenAI is to use either `temperature` or `top_p` for this purpose and not both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "SCykqsU2pnLG",
    "outputId": "6314e14f-aad4-4f92-9c24-61c420894a33"
   },
   "outputs": [],
   "source": [
    "# Needs to be set for the OpenAI API to be callable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Model and parameters (almost all that are available, logit_bias not included). Parameters model and messages are required\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "max_tokens = 1024           # Max nr of tokens to generate in the chat completion, limited by model choice\n",
    "temperature = 0.7           # Value in (0, 2). Sampling temperature for stochastic nature in response\n",
    "top_p = 1                   # Vale in (0, 1). Nucleus sampling, optional to temperature, considers tokens comprising top_p probability mass\n",
    "frequency_penalty = 0       # Value in (-2, 2). Positive value penalizes new tokens based on frequency in text so far, to decrease likelihood of repeating sentences\n",
    "presence_penalty = 0        # Value in (-2, 2). Positive value penalizes new tokens if appeared in text so far, to increase likelihood of switching to new topics\n",
    "n=1                         # How many completion options to generate for each message\n",
    "stream = False              # Boolean value which can allow for streaming response\n",
    "\n",
    "# More advanced options which allow to include function calling capability for the model itself. We will not use this functionality in the exercises below\n",
    "function_call = \"none\"      # 'auto' if call function or generate message, supply {\"name\": my_func}\n",
    "functions = [{\"name\": \"Name\", \"description\": \"semantic description of what the function(s) do\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]\n",
    "\n",
    "\n",
    "# Messages are provided as a list, similarly to the langchain method, but formatted differently\n",
    "# Other possible values for the key 'role' are 'assistant' and 'function'\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What do you know about OpenAI's mission statement?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "# API call, collected in the variable response\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "            n=n,\n",
    "            stream=stream,\n",
    "            function_call=function_call,\n",
    "            functions=functions\n",
    "        )\n",
    "\n",
    "\n",
    "# Printing the response requires a slighty less intuitive format (see below for full unformatted output)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ozae936pnLG"
   },
   "source": [
    "We can take a look at the complete response without simplifying or changing the output in any way. When you print the information from the code below, you will see that the response includes details about token usage and other aspects. These details are important as they help you estimate and collect the cost of running queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "jgO2Jz_npnLG",
    "outputId": "6863f58f-a72a-49b7-f114-b90bd19b375f"
   },
   "outputs": [],
   "source": [
    "# Printing the response variable without formatting via the print() function displays the full output from the model\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out yourself**\n",
    "\n",
    "- An important thing to keep in mind is the length of prompts in terms of tokens. Use the basic code below to estimate the number of tokens in a prompt, by using the messages dictionary and finding the length of tokens for its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# We can get the tokens by using the correct encoding name\n",
    "encoding_name = \"cl100k_base\"\n",
    "encoding_cl1000k_base = tiktoken.get_encoding(encoding_name)\n",
    "\n",
    "# For a particular model, we can get the appropropriate encoding\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "# We can check the name for this\n",
    "encoding.name\n",
    "\n",
    "tokens = encoding_cl1000k_base.encode('This is a short sentence with  seven words.')\n",
    "print(f'The number of tokens is {len(tokens)} when done with {encoding_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Soln\n",
    "# See https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb for a more complete soln\n",
    "def num_tokens_from_messages(messages):\n",
    "    \n",
    "    model='gpt-3.5-turbo-0301'\n",
    "    encoding  = tiktoken.encoding_for_model(model)\n",
    "    tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "    tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "\n",
    "    num_tokens = 0\n",
    "\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW7rSg6NqW85"
   },
   "source": [
    "## Prompt Engineering techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtRwRS0E4Zn1"
   },
   "source": [
    "Prompt Engineering (PE) is the primary vehicle for guiding generative AI models towards stable applications. It is particularly applicable to Large Language Models (LLMs) and involves formulating prompts and prompting schemas in order to retrieve appropriate responses to queries. It is essential for meaningful interactions with LLMs.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here we will explore some basics of Prompt Engineering. We showcase examples of techniques for efficiently extracting information from Large Language Models (LLMs). These techniques are widely applicable across different models, but work the best for the very large models trained on a large corpus of data.\n",
    "\n",
    "<br>\n",
    "\n",
    "In this hands-on session we aim to cover\n",
    "\n",
    "* **Why is prompting technique important?**\n",
    "  * Prompting helps in formulating the input in a way that the model can understand and respond to effectively. A well-crafted prompt can significantly improve the quality and relevance of the model's output.\n",
    "  * Through prompting, you can guide the model's responses in a particular direction or within certain boundaries. This is crucial for obtaining accurate, relevant, or safe responses.\n",
    "  * Forms the basis behind developing advanced functionalities on top of generative AI base models.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **What can we achieve?**\n",
    "  * Quickly read and summarize/extract relevant information from text source.\n",
    "  * Transform text into a format which is directly useful for you. This includes e.g. language translation or formatting the output as excel table, JSON dictionary, MarkDown or html.\n",
    "  * Get feedback and suggestions for improvement and introspection. This may include both natural language text and code.\n",
    "  * Infer sentiment, topic and logic structure in text.\n",
    "  * Generate/revise drafts for planning, policies, mails, slides and more.\n",
    "  * Brainstorming partner and ideation, an extra brain in the room.\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Important techniques**\n",
    "  * **Best practices** - Write clearly and use semantic clues rather than keyword searching. Clearly state your goals and wishes, have an intent with your promt. In some sense it is like moving back to how you perhaps intuitively wanted to interact with e.g. Google when searching internet. Or how you would ask an all-knowing oracle. Don't be afraid to interact, provide feedback and follow up when the output is not what you desired.\n",
    "  * **Role Task Format (RTF)** - Give the LLM a *role*, to set it in the right *mood*, define a clear *task* to be achieved and provide a couple of examples of how you want to *format* the output.\n",
    "  * **In Context Learning (ICL)** - Provide examples and any additional information to consider as context for your prompt. This allows for a *shallow* learning, where the LLM can learn from your prompt without updating any weights of the model. It is easier to make the bigger models accept new information in the prompt and generalize from that. The smaller models are more locked into the worldview built from training.\n",
    "  * **Chain of Thought (CoT)** - Ask the model to think through problems logically and in a step-by-step manner. This helps the model to linearize the problem into a sequence of logical considerations and often produce more accurate output.\n",
    "\n",
    "\n",
    "These techniques usually take us very far and form a basis for more advanced applications built on chaining prompts, such as e.g. [Tree of thought](https://arxiv.org/abs/2305.10601)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muDbnoDAZYQ6"
   },
   "source": [
    "### **Summarizing**\n",
    "\n",
    "Summarizing or extracting information from text sources are areas where LLMs shine particularly bright. This can be helpful for condensing lengthy news articles, generating concise summaries of meetings or reports, summarizing books or chapters for quick review or understanding and summarizing customer feedback or reviews in order to understand common sentiments or issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "kljKti7vCuUL",
    "outputId": "b4b88f41-8ac0-4372-b768-d2f9d3f6bc61"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Summarize the following conversation between a service representative and a customer in a few sentences.\n",
    "Use only the information from the conversation.\n",
    "\n",
    "Service Rep: How may I assist you today?\n",
    "Customer: I need to change the shipping address for an order.\n",
    "Service Rep: Ok, I can help you with that if the order has not been fulfilled from our warehouse yet.\n",
    "But if it has already shipped, then you will need to contact the shipping provider. Do you have the order ID?\n",
    "Customer: Yes, it's 88986367.\n",
    "Service Rep: One minute please while I pull up your order information.\n",
    "Customer: No problem\n",
    "Service Rep: Ok, it looks like your order was shipped from our warehouse 2 days ago.\n",
    "It is now in the hands of  the shipping provider, so you will need to contact them to update your delivery details.\n",
    "You can track your order with the shipping provider here: https://www.shippingprovider.com\n",
    "Customer: Sigh, ok.\n",
    "Service Rep: Is there anything else I can help you with today?\n",
    "Customer: No, thanks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "HyIugI1dCuRT",
    "outputId": "740e000c-911a-4eb9-a88f-cd4423cc4e18"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVCQZVvSaQFN"
   },
   "source": [
    "**Try it out yourself:**\n",
    "\n",
    "Try changing the prompt or temperature for the above conversation and compare the answers:\n",
    "\n",
    "\n",
    "**Summarize with a word limit:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "Summarize the following conversation between a service representative and a customer in 20 words or less.\n",
    "Use only the information from the conversation.\n",
    "\n",
    "Service Rep: How may I assist you today?\n",
    "Customer: I need to change the shipping address for an order.\n",
    "Service Rep: Ok, I can help you with that if the order has not been fulfilled from our warehouse yet.\n",
    "But if it has already shipped, then you will need to contact the shipping provider. Do you have the order ID?\n",
    "Customer: Yes, it's 88986367.\n",
    "Service Rep: One minute please while I pull up your order information.\n",
    "Customer: No problem\n",
    "Service Rep: Ok, it looks like your order was shipped from our warehouse 2 days ago.\n",
    "It is now in the hands of  the shipping provider, so you will need to contact them to update your delivery details.\n",
    "You can track your order with the shipping provider here: https://www.shippingprovider.com\n",
    "Customer: Sigh, ok.\n",
    "Service Rep: Is there anything else I can help you with today?\n",
    "Customer: No, thanks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)\n",
    "print(len(response.content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)\n",
    "print(len(response.content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "AEh3wefiCuOt",
    "outputId": "6baa7e38-6f3d-4a40-aadd-efe30b52672f"
   },
   "outputs": [],
   "source": [
    "follow_up_prompt = f\"\"\"\n",
    "Summarize the conversation in at most 30 words.\n",
    "Use only the information from the conversation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "new_messages = messages + [response, HumanMessage(content=follow_up_prompt)]\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "new_response = chat_model.invoke(new_messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(new_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIonoDYkbAyf"
   },
   "source": [
    "**Summarize with a focus on sentiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DOUCbwloCuLy",
    "outputId": "3aaba7ed-ed9c-462b-d43a-faba77468dcf"
   },
   "outputs": [],
   "source": [
    "follow_up_prompt = f\"\"\"\n",
    "Summarize the conversation in at most 30 words,\n",
    "focusing on any aspects which are relevant to customer's sentiment.\n",
    "Use only the information from the conversation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "new_messages = messages + [response, HumanMessage(content=follow_up_prompt)]\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "new_response = chat_model.invoke(new_messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(new_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iLOEI_7bnB1"
   },
   "source": [
    "### **Impersonating or defining a Role**\n",
    "Defining a role is often useful when you are looking for expert answers in a specific field or just want a personalized interaction. It can be used for company chat bots or a friendly assistant. Examples of roles to consider are: Expert, Critic, project Manager, Inventor, Journalist, entrepreneur, CFO, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lcs4F4y1CuI1",
    "outputId": "4ce9e8f1-e5f7-4276-c79a-927a94401e4b"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant defined by the following criteria:\n",
    "Role: You are ALEX, the personal assistant representative from Atlas Copco.\n",
    "Task: Always answer politely and make sure to uphold and keep in line with the core values of Atlas Copco: interaction, innovation and commitment.\n",
    "Navigating the vast landscapes of the 21st century, Atlas Copco has transformed the way individuals\n",
    "and businesses perceive and interact with leading-edge technology.\n",
    "Format: As a beacon of empowerment, you consistently challenge traditional industrial norms, advocating for personal growth, safety, and\n",
    "entrepreneurial spirit. Whether someone is curious about the latest tech advancements or the rich\n",
    "history of Atlas Copco, you have the answers. Ready to enlighten, empower, and inspire, you're the\n",
    "go-to for anyone looking to learn, build and innovate.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Can you provide a brief overview of Atlas Copco's history and its main business areas?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "PHUdybZ0chF_",
    "outputId": "8e8ad1bd-f91f-4fa0-dc00-a53347d055a4"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gwUoBOQbwTE"
   },
   "source": [
    "**Try it yourself**\n",
    "\n",
    "**Examples of other questions to ask:**\n",
    "\n",
    "* Who are you?\n",
    "* In how many countries does Atlas Copco operate?\n",
    "* What are the primary products and services offered by Atlas Copco?\n",
    "* How can I get in touch with Atlas Copco's customer support for product-related queries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = f\"\"\"\n",
    "In how many countries does Atlas Copco operate?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself**\n",
    "\n",
    "Ask multiple questions and pass the questions to the interface at the same time. Can we also get the answer in a structured form like JSON to make it easy to reinterpret the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Soln :\n",
    "user_prompt = f\"\"\"Please answer the questions below which are enclosed by triple backticks and separated by semicolons. Please present the answer in JSON format using the questions as keys \n",
    "```In how many countries does Atlas Copco operate?; What are the primary products and services offered by Atlas Copco? ```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqdFtiu5crxi"
   },
   "source": [
    "#### **Impersonating extra assignment**\n",
    "\n",
    "\n",
    "The previous example was intentionally extreme, to demonstrate just how free we are to define the atmospheric setting. In practice however it is often useful to define the role of the LLM as an expert in the area you wish to query about. This makes it easier for the LLM to navigate its knowledge base and find more accurate responses. We can additionally ask for the output to be provided as some other persona, which can be useful when the expert lingo can become too advanced.\n",
    "\n",
    "\n",
    "In this exercise we ask the LLM to be an expert astronomer, but provide its answers as if speaking to a 10-year old child.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Kakrg-gvbp4V",
    "outputId": "6bf6a46a-9f69-4183-f873-bb39a2537df0"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are an expert astronomer who is very knowledgeable about the solar system.\n",
    "Your task is to answer users questions in a satisfactory manner using clear, simple and instructive sentences.\n",
    "Shape your response as if talking to a 10-year old child.\n",
    "\n",
    "Here is an example dialogue:\n",
    "Question: How many moons does Mars have?\n",
    "Answer: Very good question. Mars has two small moons. They are called Phobos and Deimos. They are very small and irregularly shaped.\n",
    "Phobos is the larger of the two moons and is about 17 miles (27 kilometers) in diameter.\n",
    "Deimos is about 12 miles (19 kilometers) in diameter. Both moons are thought to be captured asteroids.\n",
    "Imagine them as two little buddies that always follow Mars around in space! 🪐🌕🌕\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "How many planets are there in the solar system?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "L8ZN82wYbp0S",
    "outputId": "8ad0c70a-34d5-4a7a-c713-6c1c6f580198"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLM11cCydIQ7"
   },
   "source": [
    "**Examples of other questions to ask:**\n",
    "\n",
    "* When I learned about the planets in school, there were nine. When did that change?\n",
    "* Does Pluto have any moons? What about other dwarf planets? Who chose all of these cool names?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbTdomRcpFIh"
   },
   "source": [
    "### **Writing - defining a typical task**\n",
    "\n",
    "LLMs are extraordinarily good at generating written content. This can be used for many varying purposes, such as: marketing pitches, ad generation, creating an outline for an essay, grammar correction and language translation, rewriting a text from a description or writing customized emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmwvCQyNpMHW"
   },
   "source": [
    "#### **Writing - Marketing generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_Hpc7ijYbpxk",
    "outputId": "308c5205-1d29-4bf8-99de-a3f11040f21f"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in marketing and tech product design.\n",
    "Generate a one paragraph marketing pitch from the user provided product description.\n",
    "Use only information from the provided description.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "NokiaTWS-411 Comfort Earbuds True wireless-hörlurar Svart\n",
    "Artikelnr. 5011272056 Tillv. art. nr. 8P00000143\n",
    "- Typ True wireless-hörlurar\n",
    "- Anslutningsteknik Trådlös\n",
    "- Driftstid (upp till) 9.5 h\n",
    "- Färgkategori Svart\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "nRpH2LO-pTHy",
    "outputId": "c4eee112-d73f-405b-9caa-ee90daad9d11"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK6N5eJ1pq2d"
   },
   "source": [
    "It will also take into account who you are creating the content for. If you ask it to write an email to your boss saying you will be late it will have a very different tone than if the email will go to your mom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "WTsVPY2bpTFB",
    "outputId": "52b82224-6189-49a3-ebce-4b7879eca79f"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are en expert in HR related questions and have long experience as a respected communicator.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Write a suggestion for an email to my boss, explaining that I will have to be late this morning.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "2X9KFA6WpTCO",
    "outputId": "0e0c6464-7ca6-4daf-8544-11de702d3c6d"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "rIflzHdVp6u1",
    "outputId": "587c8688-ebf6-42a4-e45c-b69f0ef5dd36"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "Write a reply to a valued customer.\n",
    "Generate a reply to thank the customer for their review.\n",
    "Make sure to use specific details from the review.\n",
    "Write in a concise and professional tone.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Customer review: \"I'm disappointed with my latest purchase. The item arrived late and was damaged.\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "songJu4rp6pQ",
    "outputId": "f1d25187-6f65-4407-efc1-4f9b28227c49"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SBwiu_xqUmH"
   },
   "source": [
    "Asking the LLM to provdie its output in a specific form or format can be very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ME8VFyE6qOHA",
    "outputId": "688ccebd-86d0-4ab6-815a-7bc47dd7e32f"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant and an expert in marketing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Create a product portfolio for a company that sells headphones and generate the output in table format with the following headers:\n",
    "\"Product class\", \"Product name\", \"Product price\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "FxhzmjpZqOEG",
    "outputId": "4a9725ec-bc9c-4de9-8e0b-f1039e21a7ed"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lMhL19gypS_l",
    "outputId": "e6131fff-cbc6-4744-da80-4267f7a28b36"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant and an expert in marketing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Create a product portfolio for a company that sells headphones and generate the output in json format,\n",
    "following exactly the below example schema. Don't include any triple backticks.\n",
    "{{\"Product class\": list of product class examples, \"Product name\": list of product name examples, \"Product price\": list of product name examples}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "oOCC6Er7bpu3",
    "outputId": "2a966f2b-1be9-4fe0-f845-6f458ca0e6b8"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JXCk0cXroTp"
   },
   "source": [
    "**We have seen some basic examples covering the concepts of Role-task-Format, Chain-of-Thought and In-Concept-Learning. Think about editing the above examples to demonstrate the concepts better and more clearly. We will also see CoT and ICL more in the subsequent sections**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pbd3QTcvtAtT"
   },
   "source": [
    "# Hands-On 2: Introduction to Embeddings & Vector index storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C-vlqsEmCkS"
   },
   "source": [
    "Some text here for introducing embeddings and vector stores ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYFAQzJ-l5w6"
   },
   "source": [
    "## Getting arXiv data & illustrating basic embedding concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1CfwSPll5kE"
   },
   "source": [
    "We will start by collecting scientific abstracts from [arXiv](https://arxiv.org). We collevct these by fetching from their new releases section, which provides the daily deluge of preprint articles in various STEM subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tTzRjjaoUn56",
    "outputId": "ddd34014-37c8-4b9b-b137-9778a4cf3bcb"
   },
   "outputs": [],
   "source": [
    "def fetch_arxiv_data(url, subject):\n",
    "    \"\"\"\n",
    "    Function for fetching articles from the arXiv. Expects a url pointing to the daily relese site of arXiv topics.\n",
    "    Returns a list of dictionaries containing 'title', 'abstract' and 'arxiv_topic'.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to retrieve data from', url)\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    papers = []\n",
    "\n",
    "    cnt_found=0\n",
    "    cnt_not_found=0\n",
    "    for item in soup.find_all('div', class_='meta'):\n",
    "      # We only extract info from articles with abstract in the listing. This includes cross-topic listings\n",
    "      try:\n",
    "        title = item.find('div', class_='list-title mathjax').text.replace('Title:', '').strip()\n",
    "        abstract = item.find('p', class_='mathjax').text.strip()\n",
    "        arxiv_topic = item.find('span', class_='primary-subject').text.strip()\n",
    "        papers.append({'title': title, 'abstract': abstract, 'arxiv_topic': arxiv_topic, 'subject': subject})\n",
    "        cnt_found+=1\n",
    "      # We do not try to get abstract from replacements\n",
    "      except:\n",
    "        #print(f\"NO ABSTRACT FOUND, DUE TO ARTICLE BEING A REPLACEMENT OF EARLIER SUBMISSION\")\n",
    "        cnt_not_found+=1\n",
    "\n",
    "    print(f\"Extracted abstract for {cnt_found} new articles from {subject}.\\nThis excludes {cnt_not_found} replacements.\")\n",
    "\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWZ9zwyzEs-V"
   },
   "source": [
    "Below we try out the function to see the output from a single article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "WIkFtDrbUnLx",
    "outputId": "1e9c3f31-c96b-4786-c970-15f97be48652"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "url = 'https://arxiv.org/list/gr-qc/new'  # URL for the General Relativity and Quantum Cosmology section\n",
    "papers = fetch_arxiv_data(url, subject=\"gr-qc\")\n",
    "\n",
    "# Print the first few papers\n",
    "for paper in papers[:1]:\n",
    "    print(\"\")\n",
    "    print(Style.BRIGHT + 'Title:' + Style.RESET_ALL, paper['title'])\n",
    "    print(Style.BRIGHT + 'Abstract:' + Style.RESET_ALL, paper['abstract'])\n",
    "    print(Style.BRIGHT + 'arXiv Topic:' + Style.RESET_ALL, paper['arxiv_topic'])\n",
    "    print(Style.BRIGHT + 'arXiv Subject:' + Style.RESET_ALL, paper['subject'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "nxrkRBb2UnOI",
    "outputId": "b6415adb-0139-451d-9073-774b2dd505d1"
   },
   "outputs": [],
   "source": [
    "# Let's do it for all arXiv subjects\n",
    "subjects = [\n",
    "    \"astro-ph\",\n",
    "    \"gr-qc\",\n",
    "    \"cond-mat\",\n",
    "    \"quant-ph\",\n",
    "    \"hep-th\",\n",
    "    \"hep-ph\",\n",
    "    \"hep-ex\",\n",
    "    \"hep-lat\",\n",
    "    \"nucl-ex\",\n",
    "    \"nucl-th\",\n",
    "    \"nlin\",\n",
    "    \"math-ph\",\n",
    "    \"math\",\n",
    "    \"cs\",\n",
    "    \"stat\",\n",
    "    \"eess\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# We collect everything in a list of dictionaries\n",
    "papers_list = []\n",
    "for subject in subjects:\n",
    "  papers_subject = fetch_arxiv_data(f\"https://arxiv.org/list/{subject}/new\", subject=subject)\n",
    "  for paper in papers_subject:\n",
    "    papers_list.append(paper)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"We extracted a total of {len(papers_list)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "F9hgqNxqlUfb",
    "outputId": "aefeb6e9-a8e5-4200-97bc-951f3c17d785"
   },
   "outputs": [],
   "source": [
    "# Print the first few papers\n",
    "for paper in papers_list[:1]:\n",
    "    print(Style.BRIGHT + 'Title:' + Style.RESET_ALL, paper['title'])\n",
    "    print(Style.BRIGHT + 'Abstract:' + Style.RESET_ALL, paper['abstract'])\n",
    "    print(Style.BRIGHT + 'arXiv Topic:' + Style.RESET_ALL, paper['arxiv_topic'])\n",
    "    print(Style.BRIGHT + 'arXiv Subject:' + Style.RESET_ALL, paper['subject'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHFv3J7euWBk"
   },
   "source": [
    "### Checking lengths of retrieved abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "LbXP7E5zfiLV",
    "outputId": "ffbc67af-9363-4488-8e25-6e622f80c858"
   },
   "outputs": [],
   "source": [
    "abstract_lengths = []\n",
    "for paper in papers_list:\n",
    "  abstract_lengths.append(len(paper[\"abstract\"]))\n",
    "\n",
    "abstract_lengths.sort(reverse=True)\n",
    "print(\"We print out the lengths of the 10 longest abstracts\")\n",
    "abstract_lengths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJclcVHPA-bp"
   },
   "source": [
    "### Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "J9X8hL-TA9An",
    "outputId": "32207676-cc46-4813-f11d-acc27385bfda"
   },
   "outputs": [],
   "source": [
    "# For embeddings we use models on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "\n",
    "# Here are some useful well ranked open source models, but they require registering and obtaining the API key, \n",
    "# currently, we will not do this, and use either the HuggingFace Embedding models below or Open AI models\n",
    "\n",
    "\n",
    "# Voyage, currently nr 1 (REQUIRES REGISTERING TO GET API KEY)\n",
    "#!pip install -q voyageai\n",
    "#import voyageai\n",
    "#from langchain.embeddings import VoyageEmbeddings\n",
    "#os.environ[\"VOYAGE_API_KEY\"] = \"...\"\n",
    "#voyageai.api_key = os.environ[\"VOYAGE_API_KEY\"]\n",
    "\n",
    "\n",
    "# Cohere, currently nr 2 (REQUIRES REGISTERING TO GET API KEY)\n",
    "#import cohere\n",
    "##Get your API key from www.cohere.com\n",
    "#os.environ[\"COHERE_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "J9X8hL-TA9An",
    "outputId": "32207676-cc46-4813-f11d-acc27385bfda"
   },
   "outputs": [],
   "source": [
    "# Open source HuggingFace embeddings, below is currently nr 3 & 12 (NO REGISTRATION REQUIRED)\n",
    "embedding_models_HF = [\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"BAAI/bge-small-en-v1.5\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "4gjqjkPFlUaW",
    "outputId": "f18d4ca8-ef26-474f-ddc4-014104e500c1"
   },
   "outputs": [],
   "source": [
    "# Helper functions for embedding chunked text using various embedding models\n",
    "#-------------------------------------------------------------------------------\n",
    "def doc_embedding(\n",
    "    embedding_model: str,\n",
    "    model_kwargs: dict={'device': 'cpu'},\n",
    "    encode_kwargs: dict={'normalize_embeddings': True},\n",
    "    cache_folder: Optional[str]=None,\n",
    "    multi_process: bool=False,\n",
    "    ) -> HuggingFaceEmbeddings:\n",
    "  \"\"\"\n",
    "  TBW...\n",
    "  \"\"\"\n",
    "  embedder = HuggingFaceEmbeddings(\n",
    "      model_name = embedding_model,\n",
    "      model_kwargs = model_kwargs,\n",
    "      encode_kwargs = encode_kwargs,\n",
    "      cache_folder = cache_folder,\n",
    "      multi_process = multi_process\n",
    "  )\n",
    "  return embedder\n",
    "\n",
    "\n",
    "\n",
    "def get_API_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "  \"\"\"This function retrieves embedding vector from text string using various models\"\"\"\n",
    "  text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "  # OpenAI embeddings\n",
    "  if model == \"text-embedding-ada-002\":\n",
    "    client = OpenAI()\n",
    "    embedding = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "  # Voyage embeddings\n",
    "  elif model == 'voyage-01':\n",
    "    voyage = VoyageEmbeddings(model=model, voyage_api_key=os.environ[\"VOYAGE_API_KEY\"])\n",
    "    embedding = voyage.embed_query(text)\n",
    "\n",
    "  # Cohere embeddings\n",
    "  elif model == \"embed-english-v3.0\":\n",
    "    co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "    embedding = co.embed([text], input_type=\"search_document\", model=model).embeddings\n",
    "\n",
    "  elif model in embedding_models_HF:\n",
    "    print(\"YES\")\n",
    "    embedder = doc_embedding(model)\n",
    "    embedding = embedder.embed_query(text)\n",
    "\n",
    "  else:\n",
    "    embedding = [None]\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayOW8GxloKDV"
   },
   "source": [
    "We will collect our abstracts into a list of Langchain Document objects. This is not necessary for doing embeddings, but will facilitate working with vector stores later on. The Document object class has the methods `page_content`, which stores the text string, and `metadata`, where additional metadata can be stored as a dictionary with key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "ehwJgI1yoJVP",
    "outputId": "ade90785-dc24-4ac5-b831-ccd4ad1db03c"
   },
   "outputs": [],
   "source": [
    "documents =  []\n",
    "\n",
    "for paper in papers_list:\n",
    "  doc  = Document(\n",
    "      page_content = paper[\"abstract\"],\n",
    "      metadata = {\"title\": paper[\"title\"], \"arxiv_topic\": paper[\"arxiv_topic\"], \"subject\": paper[\"subject\"]}\n",
    "  )\n",
    "  documents.append(doc)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oisJAgFKR3W"
   },
   "source": [
    "Let's demonstrate how an embedding works by using some open source HuggingFace embeddings for a single abstract. First let's see what the abstract looks like in plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "ZupVUDo1ufPU",
    "outputId": "fec5be6e-da14-47b0-cf79-259b8149dee8"
   },
   "outputs": [],
   "source": [
    "test_text = documents[0].page_content\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idqcaTndr6Ur"
   },
   "source": [
    "Now we call the open source HuggingFace Embedding model and check the first 10 entries of the resulting embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```The first time you run the below code snippet you will download the embedding model into memory and you will see the progress of this displayed.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "ZjlOs1EqufM-",
    "outputId": "37969e16-fbab-44a4-96a5-a15d81024c72"
   },
   "outputs": [],
   "source": [
    "embedding = get_API_embedding(test_text, model=\"BAAI/bge-small-en-v1.5\")\n",
    "embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obswLPAmW1la"
   },
   "source": [
    "We can now loop through all abstracts, embed them and add the embeddings to an embedding list. Later we will see how we can do this using a vector store to manage the retrieved embeddings along with additional metadata.\n",
    "\n",
    "**NB: This takes a couple of minutes to complete for all abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tmX1GT8ZKESE",
    "outputId": "94d10e29-8f8d-4c66-b057-021c21b0de0d"
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for document in documents:\n",
    "  embedding = get_API_embedding(document.page_content, model=\"BAAI/bge-small-en-v1.5\")\n",
    "  embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OLFee7uXE8M"
   },
   "source": [
    "Let's check the first few entries of the embedding of one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "9WAkM4N4KEPh",
    "outputId": "9ae6a1d9-e446-4bb8-b979-0ddb943b9301"
   },
   "outputs": [],
   "source": [
    "embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVaUwuSoz0A1"
   },
   "source": [
    "### Projecting embeddings using UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYQwSdUpPetr"
   },
   "source": [
    "We will now use the [UMAP](https://umap-learn.readthedocs.io/en/latest/) library for performing projections of the embedding vectors down to 2D, preserving both local and global structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MMCem1JP8vW"
   },
   "source": [
    "Let's remind ourselves what the arXiv subjects are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "W6W8jdcJ25zM",
    "outputId": "efe9fa62-3326-42ca-84c5-384ac117f86b"
   },
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkIuD8cWQFtR"
   },
   "source": [
    "We prepare the data for input to the UMAP algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "114Kfrmxz-iW",
    "outputId": "81d085e8-88b2-4fc8-cd89-f6d4c644d20e"
   },
   "outputs": [],
   "source": [
    "colors = mcp.gen_color(cmap=\"Spectral\",n=len(subjects))\n",
    "color_dict_subjects =dict(zip(subjects, colors))\n",
    "\n",
    "\n",
    "embedding_data_array = np.array(embeddings)\n",
    "print(f\"We now have an array of embeddings with shape: {embedding_data_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "F-3irvroz-fk",
    "outputId": "057e3d3f-360f-473f-f311-608da6ca54e2"
   },
   "outputs": [],
   "source": [
    "# We do the projection for several values of the n_neighbours hyperparameter\n",
    "# This is the most important hyperparameter of the UMAP algorithm\n",
    "n_neighbors = [2, 5, 15, 25, 50, 100] # 15 is default\n",
    "\n",
    "umap_results = []\n",
    "for n in n_neighbors:\n",
    "    reducer = umap.UMAP(random_state=42,\n",
    "                        n_components=2,\n",
    "                        learning_rate=1.0,\n",
    "                        min_dist=0.1,\n",
    "                        n_neighbors=n,\n",
    "                        metric='euclidean',\n",
    "                        output_metric='euclidean',\n",
    "                        target_metric='categorical',\n",
    "                        target_n_neighbors=-1,\n",
    "                        target_weight=0.5,)\n",
    "    umap_embedding = reducer.fit_transform(embeddings)\n",
    "    umap_results.append(umap_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR5dQTR-RPjJ"
   },
   "source": [
    "Let's first display the result using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "x4WsJBk81CdF",
    "outputId": "292f7805-f380-4e48-b131-0c5d6c1119f8"
   },
   "outputs": [],
   "source": [
    "# Let's choose one of the UMAP results to display\n",
    "nr_index = 5\n",
    "\n",
    "df_arxiv_umap = pd.DataFrame(np.array([umap_results[nr_index][:,0], umap_results[nr_index][:,1]]).T, columns=[\"umap-2d-one\", \"umap-2d-two\"])\n",
    "\n",
    "subjects_list = []\n",
    "for paper in papers_list:\n",
    "  subjects_list.append(paper[\"subject\"])\n",
    "df_arxiv_umap[\"y\"] = subjects_list\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"umap-2d-one\", y=\"umap-2d-two\",\n",
    "    hue=\"y\",\n",
    "    palette=color_dict_subjects,\n",
    "    data=df_arxiv_umap,\n",
    "    legend=\"full\",\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.title(f\"UMAP projection of arXiv abstracts with n_neighbors={n_neighbors[nr_index]}\", fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDTL3tXSOgc-"
   },
   "source": [
    "Let's make a more interactive plot using Plotly where we can hover over the points interactively and inspect the results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "wLTUN4eVMfAP",
    "outputId": "b3c8aeb1-e93a-471e-b380-51f01b13f481"
   },
   "outputs": [],
   "source": [
    "subjects_list = []\n",
    "for paper in papers_list:\n",
    "  subjects_list.append(paper[\"arxiv_topic\"])\n",
    "df_arxiv_umap[\"y_long\"] = subjects_list\n",
    "\n",
    "\n",
    "fig = px.scatter(df_arxiv_umap,\n",
    "                 x='umap-2d-one',\n",
    "                 y='umap-2d-two',\n",
    "                 color='y',\n",
    "                 color_discrete_map=color_dict_subjects, # Use your color dictionary\n",
    "                 hover_data=['y_long']) # This will show the category on hover\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.5)) # Adjust size and opacity similar to your seaborn plot\n",
    "fig.update_layout(legend_title_text='arXiv subject') # Customize legend title\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgnRD9JEOpzw"
   },
   "source": [
    "Tracing over the points and examining the hover labels we can see a clear clustering of physics topics and computer science topics respectively. The math topics tend to lie between these and we can see that they are closer to those topics which deal with similar lines of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y73Kd98JmhHj"
   },
   "source": [
    "> ```We can use the above plot to remove some of the topic subjects if we wish, to reduce the amount of data to embed and make the separation even more visually clear. Also consider changing color scheme.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZQZ2rIGMe9g"
   },
   "source": [
    "**Try it out yourself**\n",
    "\n",
    "- One might imagine that a similar splitting of papers by topic would have been possible by searching for certain keywords. While this maybe true, embedding models clearly outperform keywords on semantic search. Try coming up with some short sentences with similar meaning but different words, and then check that the embedding vectors have high cosine similarity to the meaning. You can use `np.dot` or use `cosine_similarity` from `sklearn.metrics.pairwise` to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Soln: \n",
    "\n",
    "sent_a = 'Company A is doing well economically' \n",
    "sent_b = 'Company A is making good revenues' \n",
    "sent_c = 'Company A has many employees'\n",
    "\n",
    "sent_a_emb = embedding = get_API_embedding(sent_a, model=\"BAAI/bge-small-en-v1.5\")\n",
    "sent_b_emb = embedding = get_API_embedding(sent_b, model=\"BAAI/bge-small-en-v1.5\")\n",
    "sent_c_emb = embedding = get_API_embedding(sent_c, model=\"BAAI/bge-small-en-v1.5\")\n",
    "profit_emb = embedding = get_API_embedding('profitable', model=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "np.dot(sent_a_emb, profit_emb), np.dot(sent_b_emb, profit_emb), np.dot(sent_c_emb, profit_emb)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([profit_emb], [sent_a_emb, sent_b_emb, sent_c_emb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MabBTvs6R35K"
   },
   "source": [
    "### Vector index store & Semantic similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_kIVHSJSCNA"
   },
   "source": [
    "Let's now examine how we can store embeddings in an indexed vector database. There are many different vector stores to choose from which all perform similarly. Here we will make use of [FAISS](https://ai.meta.com/tools/faiss/), which is an open source vector store library developed by Meta.\n",
    "\n",
    "We will then see how we can use this tool to perform a similarity search over the indexed embeddings and retrieve the most relevant article based on a query using semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNsdUsgs4Pd7"
   },
   "source": [
    "We will make use of an open source model from HuggingFace for doing the embeddings here. There are many good options to choose from of varying sizes. As we will see even quite small models perform quite well with semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j96fHlbz4hBE"
   },
   "source": [
    "> ```The first time you download the embedding model into memory you will see the progress of this displayed.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "f18ce7f1420a4918b8968a66161c62fa",
      "e2da859383ad492094c791a483f1e440",
      "457b41834ed7403baddfb9870e4f2ed8",
      "ea315748107947ae979ba911fb7bb927",
      "caf9b50f9fd1435c9e2c61948e8cca21",
      "a4f9232d8b5b42a58f778acb453ace03",
      "9b4e6652f9284dc3b9ff3b3ed781f34a",
      "e618d718046146609f5485d9e49b76df",
      "fe05f8ac72b04958941efc4ad5f53004",
      "53e2d05489b14fd098e49652f3b7b902",
      "adad4372973a48e7b8c3291145edb16c",
      "f55249a5beef43f3afadf133ba98251e",
      "570e62e16618472d8a15b4c94c2cf301",
      "d28af11715c3442c99df6240405fbb48",
      "386fd64ea48c488bb2f04cf9668b9b3b",
      "8752e44c40544cec93eed9d55b68a117",
      "a1bb604cd1864429a31211155f68d794",
      "fbb08f93abc64c4eb365d18dfdf3b0c5",
      "18983e4945ca487297292bcd17398602",
      "5b7d629a5acb4ce893627f23ec053721",
      "37202e7eec054b7488c8b66955a258c1",
      "eea26831119f4e7b82646ae8223fabe3",
      "f449134fa6764299b4599a95ee5111f9",
      "34b3be76bd2b406999d2b77491247317",
      "ec50f0a29ef34bf2923c7fb3317425e8",
      "4f8e20eaef7147a1aa671505ab3fb9af",
      "4b8351603acf4d83bce5b542c689f1b5",
      "b5be1082cbaf43698e7845a71fbfafe3",
      "65a92a20bc184ef0bec97d03ffd8b491",
      "1554a9a8e32d4c088d54b1b977434a23",
      "f25728d8c9194ce192ea3f960cc7ade9",
      "4b52caea755347e0926c5cac82224c39",
      "8097fe130fcf465c98e9bf3c919d60d6",
      "61a697d1b2f54685a24f705b82dbbf20",
      "8ed2b64a3d6744478c209104c345b1fc",
      "540b2b1de7254e4095a32489f6cca38b",
      "9255588ffbb1475697f128638ac2df8e",
      "367972865e074aeaa5209b6e61aadd40",
      "76a01a42c9ca4804a945cadf16caa423",
      "4bdd727fc789431e93a2a6d2613226df",
      "af566c6200f2439dbd5f3d9ec9f3973b",
      "88c27c5b42594b10a3e99fab47a70a6f",
      "10888eb19c17421baee6d134a3c67e14",
      "31be60fc8af14c2da1465203620cccad",
      "cd00be33574145e990ee280223deeb8e",
      "a4a912733eb446219bf24f1e804eeed6",
      "88585c1194284814a39d04e33fc484f8",
      "052f4e258e4e4642a1a3d59ae9584fbb",
      "5577bebd04e24609b179f247e88b35e2",
      "f9419dbbf2624830b0b729c6ef765493",
      "62533a0836fd463faa1a69b5632dd14d",
      "da1f6a83c641464aa955793956c6b415",
      "13fff4c430c64c11bf03f9cc3f38e70b",
      "160b21c3d2d74844ad048f72e2bf9550",
      "c00f4a1dd827439493bef1c754d2f6ba",
      "6566220e029e4c18837b48d5863bcf1b",
      "3483b08f77e64d8693e8c00859358603",
      "8f3df5e0707b4af9acd69397699deaa5",
      "3efd36964cf9471e838621c038165a5c",
      "38a384d45142494bb7fc59bb66db5be4",
      "f467840feca8425cbda1acb831d7c886",
      "75ed7bf9bb4b421fb60eef54b24e4297",
      "47a7da6ca628413ca3bb0f3c53166c44",
      "c534801373024fafa5233adf23bacb16",
      "73fd7c75cf004a5f83c3274e6b07e97b",
      "95133297df1346edadf41213f7e47c5b",
      "a665d16a57814edba0ea3fbb94a86be2",
      "0e7915fce89b4d74887e3cd0bdd5d310",
      "6eaae6225bee496a8ec5313f530492ed",
      "43db30172d26484c950ced7b29ce38d5",
      "2fcba9d757364798855859944f69ed71",
      "3fbb40c9ea53401bab3a1e2cd6905bd0",
      "db1f81abb0964a9b9f967a978651fbc2",
      "a5d9313b20534864af2070d8ca0ae1ea",
      "91d40d7cf04a4c2b893f743b28328c0c",
      "83de9b15a64a4d058d0d8d6440c4f992",
      "9153ef974a4647d3b607baa6e09639d8",
      "fe59fd8500004d1b98b5c515bcce251e",
      "e12bc364659742cb8542343ee2490e90",
      "d06db54a5ea24b478e777337351f8876",
      "79a97b97dd784091b0edba0f21529889",
      "69b493fde5e243df9be04a2f53d1bc0f",
      "e776c22363424aad96dd5497a62d0c0d",
      "c194cc600a5c49f0bfcdb7893583d88b",
      "b6eb09c71e1a4664b7a7fb037a714412",
      "69df742093da485aaea560056fa0f175",
      "6c22f3684bfd4d5c98dbb355ae86b17f",
      "b3b183e2f8a842248359a9fb8eb784d1",
      "afb6e40811dc4c7c9b71d441458bbc4c",
      "014dbc5c36fc4756b0c4c9afa2944d9a",
      "7fdb903fd43e42ae8cd7c2412f437b7f",
      "7fe5ab171ad1453d9db4a5c52174d554",
      "04c7bb6f7eee4935941aec2c1dc8b473",
      "d8fe85d3ba1343368267b793980b211b",
      "6c34decc018343a29ff677bf8662a811",
      "3ce6d63b431c44ddbb437ec726cf03a2",
      "1cdd00b6bf714c778cc0a9709679ec7f",
      "d3be0cb2955848bfb1cfd0620b8d9b85",
      "e58e8ba1ba864af78591ba8a16c2907f",
      "c0f3cd574c9048cb968296db6249e296",
      "092a59ffa1794e958002ff50e7ab8c20",
      "0b37b1de85c14e7d8c7feed79dbb2f23",
      "1098fe27e0bd48cd8c0f3d485e07a6bd",
      "482923d1135a4994aba994be6368721b",
      "99c1f6c37eeb4e78b9c6176b3a7045eb",
      "45a5c37d668447458e6ad3e33b0edb15",
      "42fbe4e68879486295c7b49563f36c89",
      "0bb791ece8164d698ac57d160c5b15d9",
      "ce982126877d4966bd102cd2cb2ca6e3",
      "7414e9f9cdcf4fa680f147576a9eabd8",
      "c01eb68ab60a4f76b03edcbb4d121294",
      "fc743075900a43539a5c5fd69c54f4a0",
      "9482f8d62f30447a83ce3ea07d9c119f",
      "feaa4cfeedc546ee98cc5a2ad3af184d",
      "e9aa46f7fd024750a07e837160e79127",
      "8d08615eecf843008f7d0e31e40ecf68",
      "856c668f67e247928b439e6fd0dddacb",
      "9754a92157b942c1b774e826a9838502",
      "5b836ea678134855a72e5c958495b407",
      "2158788eae4946f7befdf668e134bb63",
      "e5e1dc8491614a689121fe7a92464fb4",
      "1ea3bf388f304783b44031b91169550b",
      "c8f67d4b7146479cacf63de247796ee0",
      "7bad59322d60454581583d923320f3f9",
      "1d5d043f09744818a7460e8370b5dee6",
      "21e65b25bb8742ce976c369064b37a3c",
      "4c8192e3c9664e0ba84d34ca6c8d87e5",
      "f0cd5cdbf1794e66b3a358a984ba5e7e",
      "f588896d52d64716ac4e2c535c9cfff6",
      "bdafce766d97423ca329cbe2cc09d9dd",
      "83b4cd008d4d4552bb60a27d8daf0e04",
      "d40ba815e666498d8f2470364abaf4b9"
     ]
    },
    "id": "f2dCB_lp26IX",
    "outputId": "77bf8c5a-b91b-49e5-febf-16ff49230f03"
   },
   "outputs": [],
   "source": [
    "# For embeddings we use a top ranked open source model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
    "embedding_model = [\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"BAAI/bge-small-en-v1.5\"\n",
    "    ]\n",
    "embedding_model = embedding_models_HF[1]\n",
    "\n",
    "\n",
    "# We use the HuggingFaceEmbeddings wrapper to make an embedding object from our model\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "      model_name = embedding_model,\n",
    "      model_kwargs = {'device': 'cpu'},\n",
    "      encode_kwargs = {'normalize_embeddings': True},\n",
    "      cache_folder = f\"{embedding_model}_cache\",\n",
    "      multi_process = False\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnRjIc0w6O7d"
   },
   "source": [
    "It is then straightforward to create a FAISS vector index using our Document and HuggingFaceEmbeddings objects. With the small BGE model this will take slightly less than one minute to complete for 50 abstracts. We will therefore pick out a subset of roughly 50 abstracts here for demonstrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2FsXJyHI7D37",
    "outputId": "576db480-d9b7-4284-855f-e63c97aa061b"
   },
   "outputs": [],
   "source": [
    "# We aim to pick out about 50 abstracts from across the documents list, irrespective of how many we originally retrieved\n",
    "nr_abstracts_in_short_list = 50\n",
    "\n",
    "\n",
    "# We pick out the baove nr of abstracts evenly spaced out over our list of retrieved abstracts\n",
    "documents_short = documents[::max(1, len(documents) // nr_abstracts_in_short_list)]\n",
    "#documents_short = documents[-nr_abstracts_in_short_list:]\n",
    "\n",
    "print(f\" The short list of documents contain {len(documents_short)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "DLJkfQnH2lXx",
    "outputId": "acc47491-408d-49f1-e7f3-c9e6de44f0de"
   },
   "outputs": [],
   "source": [
    "# We embed and store embeddings for the shortened list of abstracts\n",
    "faiss_index = FAISS.from_documents(\n",
    "      documents=documents_short,\n",
    "      embedding=embedder\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRErJ7yq9WfC"
   },
   "source": [
    "Let's pick out an article and display its abstract. This will allow us to make a query which we know matches this particular abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "Ef_9-Mqn5Evh",
    "outputId": "7b4ec6e4-d760-4b45-9bca-9db33a56d1bb"
   },
   "outputs": [],
   "source": [
    "# choose a number between 0-20 to pick out one of the indexed abstracts\n",
    "abstract_nr = 25\n",
    "\n",
    "documents_short[abstract_nr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9rw62VP9nQs"
   },
   "source": [
    "Now we can construct a search query which is related to the above abstract. We then use this query to retrieve a nr of close matches from the indexed vector stored. These are retrieved as a list in sorted order, with the closest match appearing first.\n",
    "\n",
    "Let's do this by having the LLM make a query for us from the above abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "V786LgwERZUj",
    "outputId": "a5637afd-6b55-4947-dab6-40b429bc72fc"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in summarizing scientific literature.\n",
    "\"\"\"\n",
    "\n",
    "# We can try to set this quite low to make it hard for the retriever,\n",
    "# noting that if several abstracts are on a similar topic then a very short summary\n",
    "# should make it more difficult to retrieve the intended one\n",
    "max_words_for_summary = 3\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Consider carefully the abstract supplied below and construct a very brief summary of its content.\n",
    "Try to use a maximum of {max_words_for_summary} words and do not include any mathematical formulas in your summary.\n",
    "\n",
    "abstract: {documents_short[abstract_nr].page_content}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "R6m7HdrHMf8e",
    "outputId": "9afd60c5-e715-4768-cfa5-3ada26f050eb"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out yourself:\n",
    "\n",
    "Try it out by telling your model what information from the reports you want to compare, for example, their sustainability strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uDso4QrTAX3"
   },
   "source": [
    "Now we can use this short summary to try to find the correct article amongst all the ones we have embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "N-TJJRLg4Box",
    "outputId": "65c002e1-9706-4cc5-c510-75d3e21e6a1d"
   },
   "outputs": [],
   "source": [
    "# We define a generic query which incorporates the summary from the LLM\n",
    "search_query = f\"Find an article which discusses: {response.content}\"\n",
    "\n",
    "\n",
    "# Define how many similar documents you want to retrieve\n",
    "# These are returned in sorted order, with most similar placed first\n",
    "nr_hits = 5\n",
    "\n",
    "\n",
    "# Use FAISS to perform similarity search ...\n",
    "most_similar = faiss_index.similarity_search(query = search_query, k=nr_hits)\n",
    "\n",
    "\n",
    "# Lets check that the closest retrieved match is the same as abstract we used to construct the query\n",
    "if documents_short[abstract_nr].metadata[\"title\"] == most_similar[0].metadata[\"title\"]:\n",
    "  print(Style.BRIGHT + \"SUCESS! We found the correct abstract as the top ranked choice!\" + Style.RESET_ALL)\n",
    "  print(\"--\"*25)\n",
    "  print(most_similar[0])\n",
    "# In case it was not the top pick, we check if it was among the ones retrieved from the vector store\n",
    "elif documents_short[abstract_nr].metadata[\"title\"] in [most_similar[nr_hit].metadata[\"title\"] for nr_hit in range(1,nr_hits)]:\n",
    "  print(Style.BRIGHT + f\"PARTIAL SUCESS! We found the correct abstract among the top {nr_hits} ranked choices!\" + Style.RESET_ALL)\n",
    "  print(\"--\"*25)\n",
    "  print(most_similar[0])\n",
    "else:\n",
    "  print(Style.BRIGHT + \"FAILURE! We didn't retrieve the correct abstract as top choice!\" + Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSQU0jnvVW5T"
   },
   "source": [
    "We can inspect all the top ranked abstracts we retrieved and see how well they matched the summarization we used when searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sPT7kQ622lU9",
    "outputId": "ef63b565-9488-4181-e620-d56bd382bdfd"
   },
   "outputs": [],
   "source": [
    "cnt=1\n",
    "for doc in most_similar:\n",
    "  print(Style.BRIGHT + f\"Hit nr {cnt}, Title: {doc.metadata['title']}\" + Style.RESET_ALL)\n",
    "  print(doc.page_content)\n",
    "  print(\"--\"*25)\n",
    "  cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwc1UyORXcWK"
   },
   "source": [
    "If you try to play around with the above you will find that it is quite hard to get the retriever to fail based on a semantic similarity search even for a very condense summary of the abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVWlf-Uan-0q"
   },
   "source": [
    "#### Breaking the retriever?\n",
    "\n",
    "\n",
    "In order to make things a bit harder we will try to collect a series of abstracts which are all related to the same topic, but which are not necessarily recent. To this end we fetch 100 article abstracts based on the query keyword `LLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "0BYGnEsD8_GZ",
    "outputId": "a6f40979-d0a4-4f26-dedf-12ad866c5d44"
   },
   "outputs": [],
   "source": [
    "query_keyword = \"LLM\"\n",
    "url = f\"https://arxiv.org/search/?query={query_keyword}&searchtype=all&abstracts=show&order=-announced_date_first&size=100\"\n",
    "\n",
    "\n",
    "# We collect everything in a list of dictionaries\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "  print('Failed to retrieve data from', url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "papers = []\n",
    "for item in soup.find_all('li', class_='arxiv-result'):\n",
    "  title = item.find('p', class_='title is-5 mathjax').text.strip()\n",
    "  abstract = item.find('span', class_='abstract-full has-text-grey-dark mathjax').text.strip()\n",
    "  papers.append({\"title\": title, \"abstract\": abstract})\n",
    "\n",
    "print(\"\")\n",
    "print(f\"We extracted a total of {len(papers)} abstracts on the topic of {query_keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "25XVd8gSg5_U",
    "outputId": "e30bd386-5a2e-4513-bbe0-9fcb1915571b"
   },
   "outputs": [],
   "source": [
    "documents_topic =  []\n",
    "\n",
    "for paper in papers:\n",
    "  doc  = Document(\n",
    "      page_content = paper[\"abstract\"],\n",
    "      metadata = {\"title\": paper[\"title\"]}\n",
    "  )\n",
    "  documents_topic.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwVTxPs_h-eI"
   },
   "source": [
    "We embed these documents just as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "zxUVHodGiClX",
    "outputId": "6dbc3738-8b1c-4292-c482-521aba82b345"
   },
   "outputs": [],
   "source": [
    "# We embed and store embeddings for the shortened list of abstracts\n",
    "faiss_index_topic = FAISS.from_documents(\n",
    "      documents=documents_topic,\n",
    "      embedding=embedder\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQXkIOZziDz4"
   },
   "source": [
    "And then we pick out one of the abstracts to make a query we can try to use for search retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "H3fqGqQNhkCn",
    "outputId": "bd0074c8-7eaa-42d2-e2d6-61a60ed10d17"
   },
   "outputs": [],
   "source": [
    "# choose a number between 0-99 to pick out one of the indexed abstracts\n",
    "abstract_nr = 10\n",
    "\n",
    "documents_topic[abstract_nr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "HWo4_ExORy-M",
    "outputId": "b76de7ea-7d80-45bc-aa7e-e00b6b63edbd"
   },
   "outputs": [],
   "source": [
    "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in summarizing scientific literature.\n",
    "\"\"\"\n",
    "\n",
    "# We can try to set this quite low to make it hard for the retriever,\n",
    "# noting that if several abstracts are on a similar topic then a very short summary\n",
    "# should make it more difficult to retrieve the intended one\n",
    "max_words_for_summary = 3\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Consider carefully the abstract supplied below and construct a very brief summary of its content.\n",
    "Try to use a maximum of {max_words_for_summary} words and do not include any mathematical formulas in your summary.\n",
    "\n",
    "abstract: {documents_topic[abstract_nr].page_content}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# These are collected into a messages list of\n",
    "#\n",
    "#   SystemMessage - the system prompt\n",
    "#   HumanMessage  - the user query\n",
    "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "8njbcnBghdR7",
    "outputId": "3746d53c-8977-4275-b1ce-11cbcd7f9970"
   },
   "outputs": [],
   "source": [
    "# This creates an instance of the model interface which we can subsequently call on\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# Here we collect the output from the chat model in the variable response\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# We can print out the response by calling on its content using a .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfW7HsAVhw_h"
   },
   "source": [
    "Now let's see if we can find the needle in this proverbial haystack as easily as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "z9PkPCZcRy6z",
    "outputId": "9d29ec92-9d2d-4d54-dd69-12c0828769b5"
   },
   "outputs": [],
   "source": [
    "# We define a generic query which incorporates the summary from the LLM\n",
    "search_query = f\"Find an article which discusses: {response.content}\"\n",
    "\n",
    "\n",
    "# Define how many similar documents you want to retrieve\n",
    "# These are returned in sorted order, with most similar placed first\n",
    "nr_hits = 5\n",
    "\n",
    "\n",
    "# Use FAISS to perform similarity search ...\n",
    "most_similar_topic = faiss_index_topic.similarity_search(query = search_query, k=nr_hits)\n",
    "\n",
    "\n",
    "# Lets check that the closest retrieved match is the same as abstract we used to construct the query\n",
    "if documents_topic[abstract_nr].metadata[\"title\"] == most_similar_topic[0].metadata[\"title\"]:\n",
    "  print(Style.BRIGHT + \"SUCESS! We found the correct abstract as the top ranked choice!\" + Style.RESET_ALL)\n",
    "  print(\"--\"*25)\n",
    "  print(most_similar_topic[0])\n",
    "# In case it was not the top pick, we check if it was among the ones retrieved from the vector store\n",
    "elif documents_topic[abstract_nr].metadata[\"title\"] in [most_similar_topic[nr_hit].metadata[\"title\"] for nr_hit in range(1,nr_hits)]:\n",
    "  print(Style.BRIGHT + f\"PARTIAL SUCESS! We found the correct abstract among the top {nr_hits} ranked choices!\" + Style.RESET_ALL)\n",
    "  print(\"--\"*25)\n",
    "  print(most_similar_topic[0])\n",
    "else:\n",
    "  print(Style.BRIGHT + \"FAILURE! We didn't retrieve the correct abstract as top choice!\" + Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Try it out yourself**\n",
    "\n",
    "- Try varying the abstract number (abstract_nr) to check out a different one, the summary size (max_words_for_summary) and the number of hits (hits_nr) to  study the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGVEUsv7mmd1"
   },
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work further on the sentence transformers and on the abstracts dataset. We are going to fine-tune e open source models on the dataset. Be aware that fine-tuning in this case is just a demonstration. In a real case scenario, you would use more data and more special data for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning, we need pairs of query - corresponding abstract pairs. In this example, we are going to use Chatgpt to generate queries for each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfwNHtlCjMpo"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "def generate_questions(abstract):\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert in scientific papers.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Consider carefully the following abstract of a scientific paper: {abstract}. \n",
    "    Please provide a question to this abstract. Output only the question.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "    ]\n",
    "    response = chat_model.invoke(messages)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the queries, we save them in a json in order to be able to reuse it in case we don't want to rerun the query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1JOodSJjMd0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "queries = []\n",
    "\n",
    "for paper in papers:\n",
    "    queries.append(generate_questions(paper['abstract']))\n",
    "\n",
    "abstracts_with_queries = []\n",
    "\n",
    "for query, paper in zip(queries, papers):\n",
    "    abstracts_with_queries.append({\"query\": query, \"abstract\": paper[\"abstract\"], \"title\": paper[\"title\"]})\n",
    "\n",
    "with open(\"abstracts_with_queries.json\", \"w\") as json_file:\n",
    "    json.dump(abstracts_with_queries, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your previously generated data from json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('abstracts_with_queries.json', 'r') as file:\n",
    "    abstracts_with_queries_json = json.load(file)\n",
    "\n",
    "queries = []\n",
    "abstracts = []\n",
    "for entry in abstracts_with_queries_json:\n",
    "    queries.append(entry[\"query\"])\n",
    "    abstracts.append(entry[\"abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to fine-tune an E5 model on our data. First, we need to organise the input data. Each row should have a query and a passage which is in our case, the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = []\n",
    "for query, abstract in zip(queries, abstracts):\n",
    "    train_examples.append(InputExample(texts=[query.strip(), abstract.strip()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In utilizing the MultipleNegativesRankingLoss, it's crucial to avoid duplicate entries within the batch, specifically ensuring the absence of identical queries or paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    losses,\n",
    "    models,\n",
    "    datasets,\n",
    ")\n",
    "\n",
    "word_embbedding = models.Transformer(\"intfloat/e5-small-v2\")\n",
    "pooling = models.Pooling(word_embbedding.get_word_embedding_dimension())\n",
    "e5_model = SentenceTransformer(modules=[word_embbedding, pooling])\n",
    "\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=8)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(e5_model)\n",
    "\n",
    "num_epochs = 1\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
    "e5_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we create corresponding embeddings for each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5_embeddings = e5_model.encode(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we encode our user query and get the closest abstract embedding to it based on cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_closest_abstract(query, model, embeddings):\n",
    "    target_embedding = model.encode(query)\n",
    "    similarities = [cosine_similarity([target_embedding], [emb])[0][0] for emb in embeddings]\n",
    "    return abstracts[np.argmax(similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the implications of the observed decline in GPT-4's performance?\"\n",
    "get_closest_abstract(query, e5_model, e5_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BGE is currently the best performing open source sentence transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune this model with a custum fuction that comes with its pip library. In order to use it, we need to prepare our data in a jsonl format. \n",
    "Example:\n",
    "{\"query\": \"Five women walk along a beach wearing flip-flops.\", \"pos\": [\"Some women with flip-flops on, are walking along the beach\"], \"neg\": [\"The 4 women are sitting on the beach.\", \"There was a reform in 1996.\", \"She's not going to court to clear her record.\", \"The man is talking about hawaii.\", \"A woman is standing outside.\", \"The battle was over. \", \"A group of people plays volleyball.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_except(start_range, end_range, count, exception):\n",
    "    if count > (end_range - start_range + 1):\n",
    "        return \"Count should be smaller than the range.\"\n",
    "    \n",
    "    result = []\n",
    "    while len(result) < count:\n",
    "        num = random.randint(start_range, end_range)\n",
    "        if num != exception and num not in result:\n",
    "            result.append(num)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_with_positives_and_negatives = []\n",
    "\n",
    "for counter, entry in enumerate(abstracts_with_queries_json):\n",
    "    random_indexes = generate_random_except(0, len(abstracts_with_queries_json)-1, 4, counter)\n",
    "    entries = [abstracts_with_queries_json[i] for i in random_indexes]\n",
    "    negative_list = [entry['abstract'] for entry in entries]\n",
    "    queries_with_positives_and_negatives.append({\"query\": entry[\"query\"], \"pos\": [entry[\"abstract\"]], \"neg\": negative_list})\n",
    "\n",
    "with open('queries_with_positives_and_negatives.jsonl', 'w') as jsonl_file:\n",
    "    for entry in queries_with_positives_and_negatives:\n",
    "        json.dump(entry, jsonl_file)\n",
    "        jsonl_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir bge_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the fine-tuning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun -m FlagEmbedding.baai_general_embedding.finetune.run \\\n",
    "    --output_dir bge_abstracts \\\n",
    "    --model_name_or_path BAAI/bge-small-en-v1.5 \\\n",
    "    --train_data ./queries_with_positives_and_negatives.jsonl \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --dataloader_drop_last True \\\n",
    "    --normlized True \\\n",
    "    --temperature 0.02 \\\n",
    "    --query_max_len 64 \\\n",
    "    --passage_max_len 256 \\\n",
    "    --train_group_size 2 \\\n",
    "    --logging_steps 10 \\\n",
    "    --query_instruction_for_retrieval \"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create corresponding embeddings for each abstract with our new fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "bge_model = FlagModel('./bge_abstracts/', use_fp16=True)\n",
    "bge_embeddings = bge_model.encode(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we encode our user query and get the closest abstract embedding to it based on cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the implications of the observed decline in GPT-4's performance?\"\n",
    "get_closest_abstract(query, bge_model, bge_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning the foundational bge model might boost its effectiveness for the specific task at hand, yet it could potentially cause significant decline in the model’s overall abilities outside that specific area. If we merge the fine-tuned model and the base model with LM-Cocktail, it can increase the perfomance in general tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mixed_bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U LM_Cocktail\n",
    "!pip -qqq install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You might need to restart the kernel if the following doesn't work. Observe that doing so will clear the memory so you will have to run the cell with the function definition for `get_closest_abstract` in order for the cells below to work. In addition, if you want to run the Hands-On 3 cells after this, you have to rerun the imports in the beginning of the notebook as well as setting the OpenAI API access key again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LM_Cocktail import mix_models, mix_models_with_data\n",
    "\n",
    "model = mix_models(\n",
    "    model_names_or_paths=[\"BAAI/bge-small-en-v1.5\", \"./bge_abstracts/\"],\n",
    "    model_type='encoder', \n",
    "    weights=[0.5, 0.5],\n",
    "    output_path=\"./mixed_bge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "\n",
    "mixed_model = FlagModel('./mixed_bge', use_fp16=True)\n",
    "mixed_embeddings = mixed_model.encode(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the implications of the observed decline in GPT-4's performance?\"\n",
    "get_closest_abstract(query, mixed_model, mixed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JAzRAJkmqhe"
   },
   "source": [
    "# Hands-On 3: Working against a PDF knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M66qdDIL1aSv"
   },
   "source": [
    "Here we will see a simple example of setting up a system for Retriever Augmented Generation (RAG). We use financial reports as examples and demostrate how to interact with these as a prospective investment manager in a protfolio company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdWacV2UpnLJ"
   },
   "source": [
    "We start by providing the location and names of the PDF files which we will be exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "N8E7XBwp_rho",
    "outputId": "34c24336-33d6-4414-d576-04c532eecc95"
   },
   "outputs": [],
   "source": [
    "# Absolute or relative path to folder where the PDF files are located\n",
    "DATAPATH = \"./Data/\"\n",
    "#DATAPATH = \"/content/drive/MyDrive/bootcamp_dec7/Data/\"\n",
    "\n",
    "\n",
    "\n",
    "# Name of the PDF files (without the trailing .pdf file endings)\n",
    "caterpillar_10k = \"Caterpillar-10k\"\n",
    "whirlpool_10k = \"whirlpool-10k\"\n",
    "electrolux_ann_rep = \"Electrolux-annual-report\"\n",
    "\n",
    "\n",
    "# We collect all the PDF file names in a list\n",
    "pdf_files = [\n",
    "    caterpillar_10k,\n",
    "    whirlpool_10k,\n",
    "    electrolux_ann_rep\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMOQfRPLpnLJ"
   },
   "source": [
    "> **We load and inspect the text converted PDF files and some properties below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knrc96uypnLJ"
   },
   "source": [
    "We can now load one of the PDFs and examine its content. Note that this relies on extracting the content of the PDF as text and works well for the parts which are written text in the PDF, but works less well for e.g., tables and figures. In order to handle tables and figures well we typically need to perform pre-processing steps which are more complex and tailored for the specific PDF file type we wish to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "JrykZn822F7j",
    "outputId": "394492a0-b29e-4b24-9de9-b32048a53fc4"
   },
   "outputs": [],
   "source": [
    "# Helper functions for text retrieval from documents\n",
    "#-------------------------------------------------------------------------------\n",
    "def text_from_pdf(file_path: str) -> dict:\n",
    "  \"\"\"\n",
    "  Uses the PyMuPDF library fitz module to extract raw text from a pdf.\n",
    "    https://pymupdf.readthedocs.io/en/latest/\n",
    "\n",
    "  Input:\n",
    "  file_path - path to pdf file for parsing\n",
    "\n",
    "  Output:\n",
    "  dict - Returns a dictionary with \"page_nr\" as key and extracted text as value.\n",
    "  Additional properties can be made available (see link in comment below).\n",
    "  \"\"\"\n",
    "  doc = fitz.open(file_path)\n",
    "  pages = {}\n",
    "  for page in doc:\n",
    "    # type(page) = <class 'fitz.fitz.Page'>\n",
    "    # (https://pymupdf.readthedocs.io/en/latest/page.html)\n",
    "    pages[f\"page_{page.number}\"] = page.get_text()\n",
    "  return pages\n",
    "\n",
    "def pdf_dict_to_str(pdf_dicct: dict) -> str:\n",
    "  \"\"\"Construct a single text string from text_from_pdf() output\"\"\"\n",
    "  return \"\\n\".join(pdf_dicct.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "zSZak8rJpnLJ",
    "outputId": "b5214d01-936b-466e-8509-1661b51ca7d3"
   },
   "outputs": [],
   "source": [
    "pdf_file = caterpillar_10k\n",
    "\n",
    "# Here we read in the PDF as text using the function text_from_pdf(), defined in the file helper_functions.py\n",
    "pdf_text = text_from_pdf(DATAPATH + pdf_file + \".pdf\")\n",
    "\n",
    "\n",
    "# Here we count and print out the total nr of pages in the retrieved PDF\n",
    "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}Nr of pages in pdf document:{Style.RESET_ALL} {len(pdf_text.keys())} \\n\")\n",
    "\n",
    "\n",
    "# Here we count and print out the total nr of symbols in the retrieved PDF file\n",
    "concat_text = pdf_dict_to_str(text_from_pdf(DATAPATH + pdf_file + \".pdf\"))\n",
    "print(f\"The document contains {Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}{len(concat_text)}{Style.RESET_ALL} symbols/characters\")\n",
    "\n",
    "\n",
    "# We then print an example page from the PDF\n",
    "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}\\n\\nEx text from first page:{Style.RESET_ALL}\\n\")\n",
    "print(f\"{pdf_text['page_1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ung3ivApnLJ"
   },
   "source": [
    "### OPTIONAL: The following section performs chunking of text and creates indexed embeddings for the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeM01TyzpnLJ"
   },
   "source": [
    "The chat models have a limit in the number of tokens/characters they can receive as input. This means we cannot feed them the full PDF as context. Rather we need to find the most relevant pieces of text within the PDF and use that when asking the LLM to respond to a query about the PDF.\n",
    "\n",
    "Here we split up the three PDF documents into piecewise chunks of text for embedding purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "skMvqNkzQ68N",
    "outputId": "7481d745-c01c-4480-8cd6-d2c9dedbee4e"
   },
   "outputs": [],
   "source": [
    "def TextLoader(\n",
    "    file_path: str,\n",
    "    loader: Type[PyPDFLoader | JSONLoader | None]=None,\n",
    "    jq_schema: str='.', # '.content', '.messages[].content'\n",
    "    content_key='content',\n",
    "    json_lines: bool=False,\n",
    "    txt_encoding: str=sys.getfilesystemencoding()\n",
    "    ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Converts a pdf, JSON or txt file into a langchain Document object,\n",
    "    containing methods page_content and metadata.\n",
    "    Specify type via loader, if not PyPDFLoader | JSONLoader then assumes txt\n",
    "    \"\"\"\n",
    "    pages = None\n",
    "    if loader==PyPDFLoader:\n",
    "      loader = PyPDFLoader(file_path=file_path)\n",
    "      pages = loader.load()   # .load_and_split()\n",
    "    elif loader==JSONLoader:\n",
    "      loader = JSONLoader(\n",
    "          file_path=file_path,\n",
    "          jq_schema=jq_schema,\n",
    "          content_key=content_key,\n",
    "          json_lines=json_lines\n",
    "          )\n",
    "      pages = loader.load()\n",
    "    else:\n",
    "      text_string = text_from_txt(file_path=file_path, encoding=txt_encoding)\n",
    "      pages = [Document(page_content=text_string, metadata={\"source\": file_path})]\n",
    "    return pages\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: List[Document],\n",
    "    TextSplitter: Type[CharacterTextSplitter | RecursiveCharacterTextSplitter],\n",
    "    chunk_size: int=512,\n",
    "    chunk_overlap: int=20,\n",
    "    separator: str=None,\n",
    "    ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    TBW ...\n",
    "    Output: A list of langchain.schema.document.Document objects.\n",
    "    These have methods\n",
    "      - Document.page_content [str] contains the text chunk\n",
    "      - Doumnent.metadata [dict] contains optional metadata\n",
    "    \"\"\"\n",
    "    docs = None\n",
    "    # CharacterTextSplitter\n",
    "    if separator!=None:\n",
    "      text_splitter = CharacterTextSplitter(\n",
    "      separator = separator,\n",
    "      chunk_size = chunk_size,\n",
    "      chunk_overlap  = chunk_overlap\n",
    "      )\n",
    "      docs = text_splitter.split_documents(documents)\n",
    "    #RecursiveCharacterTextSplitter\n",
    "    else:\n",
    "      text_splitter = RecursiveCharacterTextSplitter(\n",
    "          chunk_size = chunk_size,\n",
    "          chunk_overlap  = chunk_overlap\n",
    "      )\n",
    "      docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    if docs:\n",
    "      return docs\n",
    "    else:\n",
    "      print(f\"Error: could not chunk text.\")\n",
    "      return None\n",
    "\n",
    "\n",
    "def text_from_txt(\n",
    "    file_path: str,\n",
    "    encoding: str=sys.getfilesystemencoding()\n",
    "    ) -> str:\n",
    "  \"\"\"\n",
    "  Reads a txt file and returns a string object of content\n",
    "\n",
    "  Input:\n",
    "  file_path - path to txt file for parsing\n",
    "\n",
    "  Output:\n",
    "  str - Returns extracted text as str.\n",
    "  \"\"\"\n",
    "  with open(file_path, encoding=encoding) as f:\n",
    "    text = f.read()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "OSpFUPzL_tqN",
    "outputId": "3cf35087-aa53-45be-8bcd-625237279457"
   },
   "outputs": [],
   "source": [
    "# We loop over the 3 PDF files and  collect the chunked results into a Python dictionary\n",
    "chunked_pdf_files_dict = {}\n",
    "for pdf_file in pdf_files:\n",
    "    # Here we get the path to the current PDF file\n",
    "    pdf_location = DATAPATH + pdf_file + \".pdf\"\n",
    "    # The below function performs the cunking of the PDF\n",
    "    chunked_pdf = chunk_documents(\n",
    "        documents= TextLoader(file_path=pdf_location, loader=PyPDFLoader),\n",
    "        TextSplitter=RecursiveCharacterTextSplitter,\n",
    "        chunk_size=512, # 512 is the maximum Sequence length\n",
    "        chunk_overlap=20,\n",
    "        separator=None,\n",
    "        )\n",
    "    chunked_pdf_files_dict[pdf_file] = chunked_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLg9w5arpnLJ"
   },
   "source": [
    "Let's print out a sample chunk so that we can see what kind of information is available within a single chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "v3jNhQ6WpnLJ",
    "outputId": "3ac72019-b93f-4011-c07d-c290aec2c0d5"
   },
   "outputs": [],
   "source": [
    "print(chunked_pdf_files_dict[caterpillar_10k][79].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfKOwgNNpnLJ"
   },
   "source": [
    "We can also inspect a chunk which contains a table, to see what it looks like. Observe the format of the table and note that this is the kind of inromation that will be provided as input to the LLM when we query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "O4PlS1CFpnLJ",
    "outputId": "08511f49-7fc6-4a89-f242-c3adf0a5ee20"
   },
   "outputs": [],
   "source": [
    "# The below chunk contains a table from the caterpillar_10k PDF.\n",
    "print(chunked_pdf_files_dict[caterpillar_10k][104].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAls1duKpnLK"
   },
   "source": [
    "#### Performing the embedding of the chunked PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixzJbGHipnLK"
   },
   "outputs": [],
   "source": [
    "embed_pdfs = False\n",
    "save_embeddings = False\n",
    "\n",
    "# For embeddings we use the best open source model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cImvWyNpnLK"
   },
   "source": [
    "> **NB: The below code block takes around 100 minutes to complete for the three PDF's (100-150 pages each), so this has been prepared before the lecture. We have kept the code here for those interested, but during the course**\n",
    ">\n",
    "> **`DO NOT RUN THE BELOW TWO CELLS!` or `DO NOT MODIFY THE embed_pdfs = False and save_embeddings = False!`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVn3zxpE4nOZ"
   },
   "outputs": [],
   "source": [
    "def make_index_FAISS(\n",
    "    chunked_documents: List[Document],\n",
    "    embedding: HuggingFaceEmbeddings,\n",
    "    ) -> List:\n",
    "  \"\"\"Use FAISS to perform similarity search ...\"\"\"\n",
    "  faiss_index = FAISS.from_documents(\n",
    "      documents=chunked_documents,\n",
    "      embedding=embedding\n",
    "      )\n",
    "  return faiss_index\n",
    "\n",
    "\n",
    "def similarity_search_FAISS(\n",
    "    search_query: str,\n",
    "    index_store: FAISS,\n",
    "    nr_hits: int=5,\n",
    "    ) -> List:\n",
    "  \"\"\"Use FAISS to perform similarity search ...\"\"\"\n",
    "  most_similar = index_store.similarity_search(search_query, k=nr_hits)\n",
    "  return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh6n7_MrpnLK"
   },
   "outputs": [],
   "source": [
    "if embed_pdfs:\n",
    "    embedding_dict = {}\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Creating FAISS embedding for document {pdf_file} using - {embedding_model}\")\n",
    "        start = timer()\n",
    "        embedding = doc_embedding(embedding_model=embedding_model)\n",
    "        faiss_index = make_index_FAISS(chunked_documents=chunked_pdf_files_dict[pdf_file], embedding=embedding)\n",
    "        end = timer()\n",
    "        time_taken=end-start\n",
    "        print(f\"Done! Embedded vector index created after {time_taken/60:.2f} minutes\")\n",
    "        print(\"*\"*25)\n",
    "        embedding_dict[pdf_file] = faiss_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZR0NtCbpnLK"
   },
   "source": [
    "Here we save the embeddings so that we don't have to rerun the above every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXgHYu7MpnLK"
   },
   "outputs": [],
   "source": [
    "if save_embeddings:\n",
    "    for pdf_embedding in embedding_dict.keys():\n",
    "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_embedding}.faiss\"\n",
    "        embedding_dict[pdf_embedding].save_local(FAISS_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaplPKsbpnLK"
   },
   "source": [
    "Note that the above two code blocks have been run in advance in order to save the embeddings. This allows us to avoid doing this during the live session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7XzwMK-pnLK"
   },
   "source": [
    "### Loading the saved embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk-DHn1XpnLK"
   },
   "source": [
    "In order to match our query to relevant context within the PDF files we must embed the text of the PDF files into an *embedding space*. This is done by chunking the text of the PDF files and then store the chunks as indexed *embedding vectors*. This is what has been done in advance by performing the steps in the above optional section.\n",
    "\n",
    "We can then create an *embedding vector* from our query and perform a similarity operation against all of the embedded chunks, which allow us to find the most semantically similar chunks by performing a very quick operation. In this way we can quickly retrieve relevant context for our query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN1BAEEapnLK"
   },
   "source": [
    "If embeddings are already saved we can simply load them from storage. The output from running the embedding models have been saved in the four FAISS folders which you can see in the downloaded course content. This avoids repeating the embedding process which can be quite time consuming. In a production environment you typically do this kind of indexing once, or on a running schedule if you expect the documents you embed will change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_l4Ak4apnLK"
   },
   "source": [
    "> **`Note that the first time you run the below code cell block you will load the embedding model into memory and you may see progress bars running while this happens. Once the model is loaded into memory this will no longer happen.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi3wTxW2pnLK"
   },
   "outputs": [],
   "source": [
    "# For embeddings we use the best open source model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
    "embedding_model = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# We load and collect the saved embeddings in a python dictionary. This only takes ~15 seconds to load\n",
    "faiss_embeddings_dict = {}\n",
    "if not save_embeddings:\n",
    "    for pdf_file in pdf_files:\n",
    "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_file}.faiss\"\n",
    "        embedding = doc_embedding(embedding_model=embedding_model)\n",
    "        faiss_embeddings_dict[pdf_file] = FAISS.load_local(FAISS_folder_name, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gH3dIOapnLK"
   },
   "source": [
    "Let's try out making a simple query against the saved embeddings by doing a similarity search. The similarity search is a quick operation which retrieves the most semantically similar embeddings to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IrI6te5pnLL"
   },
   "outputs": [],
   "source": [
    "# We choose one of the PDF files and a relevant search query\n",
    "pdf_file = caterpillar_10k\n",
    "similarity_query = \"What are Caterpillar's core values?\"\n",
    "\n",
    "\n",
    "# We retrieve the most semantically similar chunks by doing a similarity search over the saved embeddings in our faiss_embeddings_dict\n",
    "# The argument k sets how many such chunks we wish to retrieve\n",
    "test = faiss_embeddings_dict[pdf_file].similarity_search(similarity_query, k=10)\n",
    "\n",
    "\n",
    "# We can now print out the most similar chunk. We print out both the chunk and the PDF page on which it is located.\n",
    "# The retrieved chunks are collected in order of similarity from 0 to k-1,\n",
    "# so chunk_nr = 0 correspond to the most similar chunk\n",
    "chunk_nr = 0\n",
    "print(f\"Found on page: {test[chunk_nr].metadata['page']}\\n\")\n",
    "print(test[chunk_nr].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1qkcFGJpnLL"
   },
   "source": [
    "### Querying the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9IbWl4TpnLL"
   },
   "source": [
    "We are now ready to start asking questions of our PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdn6GzoApnLL"
   },
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "# Setting up the Model\n",
    "# We're preparing to use a specific AI model by setting some initial parameters.\n",
    "# One of these parameters is 'temperature', which affects how creative or strict the model's responses will be.\n",
    "# A lower temperature like 0.0 makes the model more focused and less random in its responses.\n",
    "temperature = 0.0\n",
    "\n",
    "\n",
    "# Now we create an instance of the ChatOpenAI model, which we'll use to generate responses.\n",
    "# We're specifying which model to use ('gpt-3.5-turbo-1106') and providing our OpenAI API key to authorize access.\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    # The below parameters can be changed\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 2:\n",
    "# Defining a Prompt Function to Use the Model\n",
    "# We define a function for more easily calling the LLM using the same prompt template but with a different query and context\n",
    "def LLM_with_pdf_context(context: str, query: str, temperature: float=0.0):\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    Ignore all previous instructions. You are a helpful investment management expert.\n",
    "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
    "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
    "    Then proceed to answer in a step-by-step manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # This prompt template inputs the merged context and query for the LLM\n",
    "    user_prompt = f\"\"\"\n",
    "    I need help from an investement management expert.\n",
    "    One of our portfolio companies have handed us these brief instruction snippets as\n",
    "    context: {context}\n",
    "\n",
    "    Use only the above context and nothing else to answer the following\n",
    "    question: {query}\n",
    "    If the answer is provided in the form of a bulleted list within the context,\n",
    "    then return those instructions verbatim and do not try to rephrase them.\n",
    "    If you cannot answer based on the information in context, then do not try to answer, but instead must answer verbatim with:\n",
    "    {{There is no available information related to your query in the context!'}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt),\n",
    "    ]\n",
    "\n",
    "    response = chat_model.invoke(messages, temperature=temperature)\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IL3UqqyspnLL"
   },
   "outputs": [],
   "source": [
    "# We choose one of the PDF files and a relevant search query. This query is the same as we asked during session 1 using the AskYourPDF plugin\n",
    "pdf_file = caterpillar_10k\n",
    "query = \"What are Caterpillar's core values?\"\n",
    "\n",
    "\n",
    "# We retrieve the most similar chunks related to above search query and collect these into a context string\n",
    "faiss_index = faiss_embeddings_dict[pdf_file]\n",
    "top_hits = similarity_search_FAISS(search_query=query, nr_hits=5, index_store=faiss_index)\n",
    "context = \"\".join([document.page_content for document in top_hits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaNW0MHQpnLL"
   },
   "source": [
    "First, let's use all of the retrieved top hits as context when querying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHnd-fAPpnLL"
   },
   "outputs": [],
   "source": [
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to be more complete by providing the full page where the retrieved chunk was found rather than just the chunk. This is often useful in order to be able to get more full answers to a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO6g6UmppnLL"
   },
   "outputs": [],
   "source": [
    "# Here we get the page of the top hit and use that whole PDF page as context\n",
    "context_page = f\"\"\"{pdf_text[f'page_{top_hits[0].metadata[\"page\"]}']}\"\"\"\n",
    "\n",
    "# Use the pre-defined function but with the whole context page as context\n",
    "LLM_with_pdf_context(context_page, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LdcOA6vpnLL"
   },
   "source": [
    "### Querying the Electrolux Annual Report PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq6PlK9FpnLL"
   },
   "source": [
    "Let's try asking some questions based on the content in the Electrolux's annual report for 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR8uh34246JW"
   },
   "outputs": [],
   "source": [
    "def get_contexts(pdf: str, query: str, faiss_embeddings_dict: dict, nr_hits: int=5):\n",
    "    contexts = []\n",
    "    faiss_index = faiss_embeddings_dict[pdf]\n",
    "    top_hits = similarity_search_FAISS(search_query=query, nr_hits=nr_hits, index_store=faiss_index)\n",
    "    context = \"\".join([document.page_content for document in top_hits])\n",
    "    contexts.append(context)\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpq0VpmdpnLL"
   },
   "outputs": [],
   "source": [
    "# Set our pdf variable to the Electrolux annual report\n",
    "pdf_file = electrolux_ann_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tEKHH6WpnLL"
   },
   "outputs": [],
   "source": [
    "query = \"Which major events happened for Electrolux during 2022?\"\n",
    "\n",
    "\n",
    "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
    "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmZGBymkpnLM"
   },
   "source": [
    "**Try it out yourself:**\n",
    "\n",
    "Try it out by searching with your own query for information from the Electrolux Annual Report. You could, for example, ask questions about risks and challenges mentioned in the report or ask the model to explain Electrolux's business in Latin America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcP1wzPqpnLM"
   },
   "outputs": [],
   "source": [
    "# Write your own query and retrieve search results from that as context for the LLM\n",
    "query = \"...\"\n",
    "\n",
    "\n",
    "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
    "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-I7T4fOpnLM"
   },
   "source": [
    "### Querying the Caterpillar 10-K PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-8JNa9epnLM"
   },
   "source": [
    "Let's try asking some questions based on the content in the Caterpillar 10-K annual report for 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF25Fej1pnLM"
   },
   "outputs": [],
   "source": [
    "# Set our pdf variable to the Caterpillar 10-K report\n",
    "pdf_file = caterpillar_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPOdlELbpnLM"
   },
   "outputs": [],
   "source": [
    "# Ask a relevant query\n",
    "query = \"Which identified risks and challenges are mentioned in the report?\"\n",
    "\n",
    "\n",
    "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
    "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSFO1rHApnLM"
   },
   "source": [
    "**Tables in PDF-reports:**\n",
    "\n",
    "The Caterpillar 10-K report contains multiple tables which may cause a problem when using the text chunks for similarity retrieval. Here we can try and ask questions about information found in tables in the report. However, depending on the chunks, the model may confuse numbers found in the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8k6DoF4MpnLM"
   },
   "outputs": [],
   "source": [
    "# Let's ask a question which requires analyzing table information inside the PDF\n",
    "query = \"How many full-time employees did they have at the end of the year 2022 in Latin America?\"\n",
    "\n",
    "\n",
    "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
    "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1s3gGGUpnLM"
   },
   "source": [
    "**Try it out yourself:**\n",
    "\n",
    "Try it out by searching with your own query for information from the Caterpillar 10-K report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPdSQmz1pnLM"
   },
   "outputs": [],
   "source": [
    "# Write your own query and retrieve search results from that as context for the LLM\n",
    "query = \"...\"\n",
    "\n",
    "\n",
    "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
    "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Use our pre-defined function to get the model response\n",
    "LLM_with_pdf_context(context, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB4hLDpbpnLM"
   },
   "source": [
    "### Comparison between two PDF: Electrolux Annual Report and Whirlpool 10-K Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1faZmz7ipnLM"
   },
   "source": [
    "Let's go a bit deeper and try to compare information between two different PDF files. We can try to ask a question for which we expect there are answers in both the Electrolux annual report and the Whirlpool 10-K report. By using a query to retrieve relevant information from both of these reports we can use that as context to the LLM and ask about comparisons etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvJgdE4jpnLM"
   },
   "outputs": [],
   "source": [
    "# We define a function for more easily call the LLM using the same prompt template but with a different query and contexts\n",
    "def LLM_with_pdf_context_comparison(company1: str, company2: str, context1: str, context2: str, query: str, temperature: float=0.0):\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    Ignore all previous instructions. You are a helpful investment management expert.\n",
    "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
    "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
    "    Then proceed to answer in a step-by-step manner.\n",
    "    \"\"\"\n",
    "\n",
    "    # This prompt inouts the merged context\n",
    "    user_prompt = f\"\"\"\n",
    "    Two of our portfolio companies have handed us these brief instruction snippets as\n",
    "    contexts. Contexts for {company1}: {context1}, and for {company2}: {context2}\n",
    "\n",
    "    Use only the above contexts and nothing else to answer the following\n",
    "    query: {query}\n",
    "    If the answer is provided in the form of a bulleted list within the context,\n",
    "    then return those instructions verbatim and do not try to rephrase them.\n",
    "    If you cannot answer based on the information in context, then do not try to answer, but instead must answer verbatim with:\n",
    "    {{There is no available information related to your query in the context!'}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt),\n",
    "    ]\n",
    "\n",
    "    response = chat_model.invoke(messages, temperature=temperature)\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RY255ZHpnLM"
   },
   "source": [
    "With the help of the above function we can pose queries which prompts the LLM to compare the information in different PDF's by creating separate contexts. Here, we compare the information in the Electrolux Annual report and Whirlpool 10-K report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeIisKl4pnLM"
   },
   "outputs": [],
   "source": [
    "# Let's make a query which we would like to pose to both the Whirlpool and the Electrolux PDF files\n",
    "query = \"Which key business strategies were mentioned in the report?\"\n",
    "\n",
    "\n",
    "# Using the above query, we can fetch relevant contexts from both PDF files\n",
    "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAGaEqmypnLM"
   },
   "outputs": [],
   "source": [
    "# Given the above retrieved contexts we can now ask a comparative question between the companies\n",
    "chat_query = \"\"\"\n",
    "You will be provided contextual information which you should analyze very carefully.\n",
    "Think about the problem in a step by step manner.\n",
    "Consider the following key business strategies for Whirlpool and Electrolux. \n",
    "Compare them as much as possible. \n",
    "Which strategies are similar and which are different?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz8EQz75pnLM"
   },
   "outputs": [],
   "source": [
    "# Use our pre-defined function together with the names of the companies and their context\n",
    "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kq0shQDpnLN"
   },
   "source": [
    "**Try it out yourself:**\n",
    "\n",
    "Try it out by telling your model what information from the reports you want to compare, for example, their sustainability strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzxAQRetpnLN"
   },
   "outputs": [],
   "source": [
    "# Write your own query on what you like to pose to both the Whirlpool and the Electrolux PDF files\n",
    "query = \"...\"\n",
    "\n",
    "\n",
    "# Using the above query, we can fetch relevant contexts from both PDF files\n",
    "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
    "\n",
    "\n",
    "# Change this query to ask the model to compare ... for the companies.\n",
    "chat_query = \"This is the ... for Whirlpool and Electrolux. Compare them.\"\n",
    "\n",
    "\n",
    "# Use our pre-defined function together with the names of the companies and their context\n",
    "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXjr0_OdpnLN"
   },
   "source": [
    "### End of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sSdr-YPSpnLI"
   ],
   "provenance": []
  },
  "interpreter": {
   "hash": "cb01f1dbab416340528beb9003d373e701839649ec4e19c51caaaa46d79e9db4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "014dbc5c36fc4756b0c4c9afa2944d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8fe85d3ba1343368267b793980b211b",
      "placeholder": "​",
      "style": "IPY_MODEL_6c34decc018343a29ff677bf8662a811",
      "value": "tokenizer.json: 100%"
     }
    },
    "04c7bb6f7eee4935941aec2c1dc8b473": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "052f4e258e4e4642a1a3d59ae9584fbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_160b21c3d2d74844ad048f72e2bf9550",
      "placeholder": "​",
      "style": "IPY_MODEL_c00f4a1dd827439493bef1c754d2f6ba",
      "value": " 124/124 [00:00&lt;00:00, 6.20kB/s]"
     }
    },
    "092a59ffa1794e958002ff50e7ab8c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99c1f6c37eeb4e78b9c6176b3a7045eb",
      "placeholder": "​",
      "style": "IPY_MODEL_45a5c37d668447458e6ad3e33b0edb15",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "0b37b1de85c14e7d8c7feed79dbb2f23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42fbe4e68879486295c7b49563f36c89",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0bb791ece8164d698ac57d160c5b15d9",
      "value": 366
     }
    },
    "0bb791ece8164d698ac57d160c5b15d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e7915fce89b4d74887e3cd0bdd5d310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fbb40c9ea53401bab3a1e2cd6905bd0",
      "placeholder": "​",
      "style": "IPY_MODEL_db1f81abb0964a9b9f967a978651fbc2",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "10888eb19c17421baee6d134a3c67e14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1098fe27e0bd48cd8c0f3d485e07a6bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce982126877d4966bd102cd2cb2ca6e3",
      "placeholder": "​",
      "style": "IPY_MODEL_7414e9f9cdcf4fa680f147576a9eabd8",
      "value": " 366/366 [00:00&lt;00:00, 15.2kB/s]"
     }
    },
    "13fff4c430c64c11bf03f9cc3f38e70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1554a9a8e32d4c088d54b1b977434a23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "160b21c3d2d74844ad048f72e2bf9550": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18983e4945ca487297292bcd17398602": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cdd00b6bf714c778cc0a9709679ec7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1d5d043f09744818a7460e8370b5dee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83b4cd008d4d4552bb60a27d8daf0e04",
      "placeholder": "​",
      "style": "IPY_MODEL_d40ba815e666498d8f2470364abaf4b9",
      "value": " 349/349 [00:00&lt;00:00, 12.2kB/s]"
     }
    },
    "1ea3bf388f304783b44031b91169550b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8f67d4b7146479cacf63de247796ee0",
       "IPY_MODEL_7bad59322d60454581583d923320f3f9",
       "IPY_MODEL_1d5d043f09744818a7460e8370b5dee6"
      ],
      "layout": "IPY_MODEL_21e65b25bb8742ce976c369064b37a3c"
     }
    },
    "2158788eae4946f7befdf668e134bb63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e65b25bb8742ce976c369064b37a3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fcba9d757364798855859944f69ed71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31be60fc8af14c2da1465203620cccad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3483b08f77e64d8693e8c00859358603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f467840feca8425cbda1acb831d7c886",
      "placeholder": "​",
      "style": "IPY_MODEL_75ed7bf9bb4b421fb60eef54b24e4297",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "34b3be76bd2b406999d2b77491247317": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5be1082cbaf43698e7845a71fbfafe3",
      "placeholder": "​",
      "style": "IPY_MODEL_65a92a20bc184ef0bec97d03ffd8b491",
      "value": "README.md: 100%"
     }
    },
    "367972865e074aeaa5209b6e61aadd40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37202e7eec054b7488c8b66955a258c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "386fd64ea48c488bb2f04cf9668b9b3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37202e7eec054b7488c8b66955a258c1",
      "placeholder": "​",
      "style": "IPY_MODEL_eea26831119f4e7b82646ae8223fabe3",
      "value": " 190/190 [00:00&lt;00:00, 9.57kB/s]"
     }
    },
    "38a384d45142494bb7fc59bb66db5be4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ce6d63b431c44ddbb437ec726cf03a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3efd36964cf9471e838621c038165a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73fd7c75cf004a5f83c3274e6b07e97b",
      "placeholder": "​",
      "style": "IPY_MODEL_95133297df1346edadf41213f7e47c5b",
      "value": " 134M/134M [00:01&lt;00:00, 156MB/s]"
     }
    },
    "3fbb40c9ea53401bab3a1e2cd6905bd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42fbe4e68879486295c7b49563f36c89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43db30172d26484c950ced7b29ce38d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83de9b15a64a4d058d0d8d6440c4f992",
      "placeholder": "​",
      "style": "IPY_MODEL_9153ef974a4647d3b607baa6e09639d8",
      "value": " 52.0/52.0 [00:00&lt;00:00, 1.98kB/s]"
     }
    },
    "457b41834ed7403baddfb9870e4f2ed8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e618d718046146609f5485d9e49b76df",
      "max": 1519,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe05f8ac72b04958941efc4ad5f53004",
      "value": 1519
     }
    },
    "45a5c37d668447458e6ad3e33b0edb15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47a7da6ca628413ca3bb0f3c53166c44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "482923d1135a4994aba994be6368721b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b52caea755347e0926c5cac82224c39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b8351603acf4d83bce5b542c689f1b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bdd727fc789431e93a2a6d2613226df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c8192e3c9664e0ba84d34ca6c8d87e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8e20eaef7147a1aa671505ab3fb9af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b52caea755347e0926c5cac82224c39",
      "placeholder": "​",
      "style": "IPY_MODEL_8097fe130fcf465c98e9bf3c919d60d6",
      "value": " 90.3k/90.3k [00:00&lt;00:00, 4.14MB/s]"
     }
    },
    "53e2d05489b14fd098e49652f3b7b902": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "540b2b1de7254e4095a32489f6cca38b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af566c6200f2439dbd5f3d9ec9f3973b",
      "max": 743,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88c27c5b42594b10a3e99fab47a70a6f",
      "value": 743
     }
    },
    "5577bebd04e24609b179f247e88b35e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "570e62e16618472d8a15b4c94c2cf301": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1bb604cd1864429a31211155f68d794",
      "placeholder": "​",
      "style": "IPY_MODEL_fbb08f93abc64c4eb365d18dfdf3b0c5",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "5b7d629a5acb4ce893627f23ec053721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b836ea678134855a72e5c958495b407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61a697d1b2f54685a24f705b82dbbf20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ed2b64a3d6744478c209104c345b1fc",
       "IPY_MODEL_540b2b1de7254e4095a32489f6cca38b",
       "IPY_MODEL_9255588ffbb1475697f128638ac2df8e"
      ],
      "layout": "IPY_MODEL_367972865e074aeaa5209b6e61aadd40"
     }
    },
    "62533a0836fd463faa1a69b5632dd14d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6566220e029e4c18837b48d5863bcf1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3483b08f77e64d8693e8c00859358603",
       "IPY_MODEL_8f3df5e0707b4af9acd69397699deaa5",
       "IPY_MODEL_3efd36964cf9471e838621c038165a5c"
      ],
      "layout": "IPY_MODEL_38a384d45142494bb7fc59bb66db5be4"
     }
    },
    "65a92a20bc184ef0bec97d03ffd8b491": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69b493fde5e243df9be04a2f53d1bc0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69df742093da485aaea560056fa0f175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c22f3684bfd4d5c98dbb355ae86b17f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c34decc018343a29ff677bf8662a811": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eaae6225bee496a8ec5313f530492ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5d9313b20534864af2070d8ca0ae1ea",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91d40d7cf04a4c2b893f743b28328c0c",
      "value": 52
     }
    },
    "73fd7c75cf004a5f83c3274e6b07e97b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7414e9f9cdcf4fa680f147576a9eabd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75ed7bf9bb4b421fb60eef54b24e4297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76a01a42c9ca4804a945cadf16caa423": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79a97b97dd784091b0edba0f21529889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c22f3684bfd4d5c98dbb355ae86b17f",
      "placeholder": "​",
      "style": "IPY_MODEL_b3b183e2f8a842248359a9fb8eb784d1",
      "value": " 125/125 [00:00&lt;00:00, 3.71kB/s]"
     }
    },
    "7bad59322d60454581583d923320f3f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f588896d52d64716ac4e2c535c9cfff6",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdafce766d97423ca329cbe2cc09d9dd",
      "value": 349
     }
    },
    "7fdb903fd43e42ae8cd7c2412f437b7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ce6d63b431c44ddbb437ec726cf03a2",
      "max": 711396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1cdd00b6bf714c778cc0a9709679ec7f",
      "value": 711396
     }
    },
    "7fe5ab171ad1453d9db4a5c52174d554": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3be0cb2955848bfb1cfd0620b8d9b85",
      "placeholder": "​",
      "style": "IPY_MODEL_e58e8ba1ba864af78591ba8a16c2907f",
      "value": " 711k/711k [00:00&lt;00:00, 18.7MB/s]"
     }
    },
    "8097fe130fcf465c98e9bf3c919d60d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83b4cd008d4d4552bb60a27d8daf0e04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83de9b15a64a4d058d0d8d6440c4f992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "856c668f67e247928b439e6fd0dddacb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8752e44c40544cec93eed9d55b68a117": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88585c1194284814a39d04e33fc484f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da1f6a83c641464aa955793956c6b415",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13fff4c430c64c11bf03f9cc3f38e70b",
      "value": 124
     }
    },
    "88c27c5b42594b10a3e99fab47a70a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d08615eecf843008f7d0e31e40ecf68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ed2b64a3d6744478c209104c345b1fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76a01a42c9ca4804a945cadf16caa423",
      "placeholder": "​",
      "style": "IPY_MODEL_4bdd727fc789431e93a2a6d2613226df",
      "value": "config.json: 100%"
     }
    },
    "8f3df5e0707b4af9acd69397699deaa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47a7da6ca628413ca3bb0f3c53166c44",
      "max": 133508397,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c534801373024fafa5233adf23bacb16",
      "value": 133508397
     }
    },
    "9153ef974a4647d3b607baa6e09639d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91d40d7cf04a4c2b893f743b28328c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9255588ffbb1475697f128638ac2df8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10888eb19c17421baee6d134a3c67e14",
      "placeholder": "​",
      "style": "IPY_MODEL_31be60fc8af14c2da1465203620cccad",
      "value": " 743/743 [00:00&lt;00:00, 37.1kB/s]"
     }
    },
    "9482f8d62f30447a83ce3ea07d9c119f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9754a92157b942c1b774e826a9838502",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b836ea678134855a72e5c958495b407",
      "value": 231508
     }
    },
    "95133297df1346edadf41213f7e47c5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9754a92157b942c1b774e826a9838502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99c1f6c37eeb4e78b9c6176b3a7045eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b4e6652f9284dc3b9ff3b3ed781f34a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1bb604cd1864429a31211155f68d794": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4a912733eb446219bf24f1e804eeed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9419dbbf2624830b0b729c6ef765493",
      "placeholder": "​",
      "style": "IPY_MODEL_62533a0836fd463faa1a69b5632dd14d",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "a4f9232d8b5b42a58f778acb453ace03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5d9313b20534864af2070d8ca0ae1ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a665d16a57814edba0ea3fbb94a86be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e7915fce89b4d74887e3cd0bdd5d310",
       "IPY_MODEL_6eaae6225bee496a8ec5313f530492ed",
       "IPY_MODEL_43db30172d26484c950ced7b29ce38d5"
      ],
      "layout": "IPY_MODEL_2fcba9d757364798855859944f69ed71"
     }
    },
    "adad4372973a48e7b8c3291145edb16c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af566c6200f2439dbd5f3d9ec9f3973b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afb6e40811dc4c7c9b71d441458bbc4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_014dbc5c36fc4756b0c4c9afa2944d9a",
       "IPY_MODEL_7fdb903fd43e42ae8cd7c2412f437b7f",
       "IPY_MODEL_7fe5ab171ad1453d9db4a5c52174d554"
      ],
      "layout": "IPY_MODEL_04c7bb6f7eee4935941aec2c1dc8b473"
     }
    },
    "b3b183e2f8a842248359a9fb8eb784d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5be1082cbaf43698e7845a71fbfafe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6eb09c71e1a4664b7a7fb037a714412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdafce766d97423ca329cbe2cc09d9dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c00f4a1dd827439493bef1c754d2f6ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c01eb68ab60a4f76b03edcbb4d121294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc743075900a43539a5c5fd69c54f4a0",
       "IPY_MODEL_9482f8d62f30447a83ce3ea07d9c119f",
       "IPY_MODEL_feaa4cfeedc546ee98cc5a2ad3af184d"
      ],
      "layout": "IPY_MODEL_e9aa46f7fd024750a07e837160e79127"
     }
    },
    "c0f3cd574c9048cb968296db6249e296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_092a59ffa1794e958002ff50e7ab8c20",
       "IPY_MODEL_0b37b1de85c14e7d8c7feed79dbb2f23",
       "IPY_MODEL_1098fe27e0bd48cd8c0f3d485e07a6bd"
      ],
      "layout": "IPY_MODEL_482923d1135a4994aba994be6368721b"
     }
    },
    "c194cc600a5c49f0bfcdb7893583d88b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c534801373024fafa5233adf23bacb16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8f67d4b7146479cacf63de247796ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c8192e3c9664e0ba84d34ca6c8d87e5",
      "placeholder": "​",
      "style": "IPY_MODEL_f0cd5cdbf1794e66b3a358a984ba5e7e",
      "value": "modules.json: 100%"
     }
    },
    "caf9b50f9fd1435c9e2c61948e8cca21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd00be33574145e990ee280223deeb8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4a912733eb446219bf24f1e804eeed6",
       "IPY_MODEL_88585c1194284814a39d04e33fc484f8",
       "IPY_MODEL_052f4e258e4e4642a1a3d59ae9584fbb"
      ],
      "layout": "IPY_MODEL_5577bebd04e24609b179f247e88b35e2"
     }
    },
    "ce982126877d4966bd102cd2cb2ca6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d06db54a5ea24b478e777337351f8876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6eb09c71e1a4664b7a7fb037a714412",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69df742093da485aaea560056fa0f175",
      "value": 125
     }
    },
    "d28af11715c3442c99df6240405fbb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18983e4945ca487297292bcd17398602",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b7d629a5acb4ce893627f23ec053721",
      "value": 190
     }
    },
    "d3be0cb2955848bfb1cfd0620b8d9b85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d40ba815e666498d8f2470364abaf4b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8fe85d3ba1343368267b793980b211b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da1f6a83c641464aa955793956c6b415": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1f81abb0964a9b9f967a978651fbc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e12bc364659742cb8542343ee2490e90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e776c22363424aad96dd5497a62d0c0d",
      "placeholder": "​",
      "style": "IPY_MODEL_c194cc600a5c49f0bfcdb7893583d88b",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "e2da859383ad492094c791a483f1e440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4f9232d8b5b42a58f778acb453ace03",
      "placeholder": "​",
      "style": "IPY_MODEL_9b4e6652f9284dc3b9ff3b3ed781f34a",
      "value": ".gitattributes: 100%"
     }
    },
    "e58e8ba1ba864af78591ba8a16c2907f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5e1dc8491614a689121fe7a92464fb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e618d718046146609f5485d9e49b76df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e776c22363424aad96dd5497a62d0c0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9aa46f7fd024750a07e837160e79127": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea315748107947ae979ba911fb7bb927": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53e2d05489b14fd098e49652f3b7b902",
      "placeholder": "​",
      "style": "IPY_MODEL_adad4372973a48e7b8c3291145edb16c",
      "value": " 1.52k/1.52k [00:00&lt;00:00, 42.9kB/s]"
     }
    },
    "ec50f0a29ef34bf2923c7fb3317425e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1554a9a8e32d4c088d54b1b977434a23",
      "max": 90346,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f25728d8c9194ce192ea3f960cc7ade9",
      "value": 90346
     }
    },
    "eea26831119f4e7b82646ae8223fabe3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0cd5cdbf1794e66b3a358a984ba5e7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f18ce7f1420a4918b8968a66161c62fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2da859383ad492094c791a483f1e440",
       "IPY_MODEL_457b41834ed7403baddfb9870e4f2ed8",
       "IPY_MODEL_ea315748107947ae979ba911fb7bb927"
      ],
      "layout": "IPY_MODEL_caf9b50f9fd1435c9e2c61948e8cca21"
     }
    },
    "f25728d8c9194ce192ea3f960cc7ade9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f449134fa6764299b4599a95ee5111f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34b3be76bd2b406999d2b77491247317",
       "IPY_MODEL_ec50f0a29ef34bf2923c7fb3317425e8",
       "IPY_MODEL_4f8e20eaef7147a1aa671505ab3fb9af"
      ],
      "layout": "IPY_MODEL_4b8351603acf4d83bce5b542c689f1b5"
     }
    },
    "f467840feca8425cbda1acb831d7c886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f55249a5beef43f3afadf133ba98251e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_570e62e16618472d8a15b4c94c2cf301",
       "IPY_MODEL_d28af11715c3442c99df6240405fbb48",
       "IPY_MODEL_386fd64ea48c488bb2f04cf9668b9b3b"
      ],
      "layout": "IPY_MODEL_8752e44c40544cec93eed9d55b68a117"
     }
    },
    "f588896d52d64716ac4e2c535c9cfff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9419dbbf2624830b0b729c6ef765493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbb08f93abc64c4eb365d18dfdf3b0c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc743075900a43539a5c5fd69c54f4a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d08615eecf843008f7d0e31e40ecf68",
      "placeholder": "​",
      "style": "IPY_MODEL_856c668f67e247928b439e6fd0dddacb",
      "value": "vocab.txt: 100%"
     }
    },
    "fe05f8ac72b04958941efc4ad5f53004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe59fd8500004d1b98b5c515bcce251e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e12bc364659742cb8542343ee2490e90",
       "IPY_MODEL_d06db54a5ea24b478e777337351f8876",
       "IPY_MODEL_79a97b97dd784091b0edba0f21529889"
      ],
      "layout": "IPY_MODEL_69b493fde5e243df9be04a2f53d1bc0f"
     }
    },
    "feaa4cfeedc546ee98cc5a2ad3af184d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2158788eae4946f7befdf668e134bb63",
      "placeholder": "​",
      "style": "IPY_MODEL_e5e1dc8491614a689121fe7a92464fb4",
      "value": " 232k/232k [00:00&lt;00:00, 6.13MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
